{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Test CI If you are building distributed applications Ray Core provides a simple, universal API for building distributed applications. Ray accomplishes this mission by: Providing simple primitives for building and running distributed applications. Enabling end users to parallelize single machine code, with little to zero code changes. Including a large ecosystem of applications, libraries, and tools on top of the core Ray to enable complex applications. If you are building machine learning solutions On top of Ray Core are several libraries for solving problems in machine learning: Ray Data (beta) : distributed data loading and compute Ray Train : distributed deep learning Ray Tune : scalable hyperparameter tuning Ray RLlib : industry-grade reinforcement learning As well as libraries for taking ML and distributed apps to production: Ray Serve : scalable and programmable serving Ray Workflows (alpha) : fast, durable application flows There are also many community integrations with Ray, including Dask , MARS , Modin , Horovod , Hugging Face , Scikit-learn , and others. Check out the full list of Ray distributed libraries here . If you are deploying Ray on your infrastructure TODO Getting Involved Ray is more than a framework for distributed applications but also an active community of developers, researchers, and folks that love machine learning. Here's a list of tips for getting involved with the Ray community: Join our community Slack to discuss Ray! Star and follow us on GitHub . To post questions or feature requests, check out the Discussion Board . Follow us and spread the word on Twitter . Join our Meetup Group to connect with others in the community. Use the [ray] tag on StackOverflow to ask and answer questions about Ray usage. If you're interested in contributing to Ray, visit our page on Getting Involved to read about the contribution process and see what you can work on!","title":"Welcome to Ray docs"},{"location":"#test-ci","text":"","title":"Test CI"},{"location":"#if-you-are-building-distributed-applications","text":"Ray Core provides a simple, universal API for building distributed applications. Ray accomplishes this mission by: Providing simple primitives for building and running distributed applications. Enabling end users to parallelize single machine code, with little to zero code changes. Including a large ecosystem of applications, libraries, and tools on top of the core Ray to enable complex applications.","title":"If you are building distributed applications"},{"location":"#if-you-are-building-machine-learning-solutions","text":"On top of Ray Core are several libraries for solving problems in machine learning: Ray Data (beta) : distributed data loading and compute Ray Train : distributed deep learning Ray Tune : scalable hyperparameter tuning Ray RLlib : industry-grade reinforcement learning As well as libraries for taking ML and distributed apps to production: Ray Serve : scalable and programmable serving Ray Workflows (alpha) : fast, durable application flows There are also many community integrations with Ray, including Dask , MARS , Modin , Horovod , Hugging Face , Scikit-learn , and others. Check out the full list of Ray distributed libraries here .","title":"If you are building machine learning solutions"},{"location":"#if-you-are-deploying-ray-on-your-infrastructure","text":"TODO","title":"If you are deploying Ray on your infrastructure"},{"location":"#getting-involved","text":"Ray is more than a framework for distributed applications but also an active community of developers, researchers, and folks that love machine learning. Here's a list of tips for getting involved with the Ray community: Join our community Slack to discuss Ray! Star and follow us on GitHub . To post questions or feature requests, check out the Discussion Board . Follow us and spread the word on Twitter . Join our Meetup Group to connect with others in the community. Use the [ray] tag on StackOverflow to ask and answer questions about Ray usage. If you're interested in contributing to Ray, visit our page on Getting Involved to read about the contribution process and see what you can work on!","title":"Getting Involved"},{"location":"api-references/ray-core/","text":"ray core","title":"API references"},{"location":"api-references/ray-datasets/","text":"ray core","title":"Ray datasets"},{"location":"api-references/ray-rllib/","text":"ray core","title":"Ray rllib"},{"location":"api-references/ray-serve/","text":"ray core","title":"Ray serve"},{"location":"api-references/ray-train/","text":"ray core","title":"Ray train"},{"location":"api-references/ray-tune/","text":"ray core","title":"Ray tune"},{"location":"api-references/ray-workflows/","text":"ray core","title":"Ray workflows"},{"location":"contributor-guide/getting-involved/","text":"getting-involved","title":"Getting involved"},{"location":"ecosystem/integrations/integrations/","text":"","title":"Integrations"},{"location":"ecosystem/integrations/joblib/","text":"","title":"Joblib"},{"location":"examples/","text":"","title":"Examples"},{"location":"overview-ray/more-information/","text":"Here are some talks, papers, and press coverage involving Ray and its libraries. Please raise an issue if any of the below links are broken, or if you\\'d like to add your own talk! Blog and Press Modern Parallel and Distributed Python: A Quick Tutorial on Ray Why Every Python Developer Will Love Ray Ray: A Distributed System for AI (BAIR) 10x Faster Parallel Python Without Python Multiprocessing Implementing A Parameter Server in 15 Lines of Python with Ray Ray Distributed AI Framework Curriculum RayOnSpark: Running Emerging AI Applications on Big Data Clusters with Ray and Analytics Zoo First user tips for Ray [Tune] Tune: a Python library for fast hyperparameter tuning at any scale [Tune] Cutting edge hyperparameter tuning with Ray Tune [RLlib] New Library Targets High Speed Reinforcement Learning [RLlib] Scaling Multi Agent Reinforcement Learning [RLlib] Functional RL with Keras and Tensorflow Eager [Modin] How to Speed up Pandas by 4x with one line of code [Modin] Quick Tip -- Speed up Pandas using Modin Ray Blog Talks (Videos) [Unifying Large Scale Data Preprocessing and Machine Learning Pipelines with Ray Datasets | PyData 2021][] (slides) [Programming at any Scale with Ray | SF Python Meetup Sept 2019] [Ray for Reinforcement Learning | Data Council 2019] Scaling Interactive Pandas Workflows with Modin [Ray: A Distributed Execution Framework for AI | SciPy 2018] [Ray: A Cluster Computing Engine for Reinforcement Learning Applications | Spark Summit] [RLlib: Ray Reinforcement Learning Library | RISECamp 2018] [Enabling Composition in Distributed Reinforcement Learning | Spark Summit 2018] [Tune: Distributed Hyperparameter Search | RISECamp 2018] Slides Talk given at UC Berkeley DS100 Talk given in October 2019 [Tune] Talk given at RISECamp 2019 Papers Ray 1.0 Architecture whitepaper (new) Ray Design Patterns (new) RLlib paper RLlib flow paper Tune paper Older papers: Ray paper Ray HotOS paper","title":"More information"},{"location":"overview-ray/more-information/#blog-and-press","text":"Modern Parallel and Distributed Python: A Quick Tutorial on Ray Why Every Python Developer Will Love Ray Ray: A Distributed System for AI (BAIR) 10x Faster Parallel Python Without Python Multiprocessing Implementing A Parameter Server in 15 Lines of Python with Ray Ray Distributed AI Framework Curriculum RayOnSpark: Running Emerging AI Applications on Big Data Clusters with Ray and Analytics Zoo First user tips for Ray [Tune] Tune: a Python library for fast hyperparameter tuning at any scale [Tune] Cutting edge hyperparameter tuning with Ray Tune [RLlib] New Library Targets High Speed Reinforcement Learning [RLlib] Scaling Multi Agent Reinforcement Learning [RLlib] Functional RL with Keras and Tensorflow Eager [Modin] How to Speed up Pandas by 4x with one line of code [Modin] Quick Tip -- Speed up Pandas using Modin Ray Blog","title":"Blog and Press"},{"location":"overview-ray/more-information/#talks-videos","text":"[Unifying Large Scale Data Preprocessing and Machine Learning Pipelines with Ray Datasets | PyData 2021][] (slides) [Programming at any Scale with Ray | SF Python Meetup Sept 2019] [Ray for Reinforcement Learning | Data Council 2019] Scaling Interactive Pandas Workflows with Modin [Ray: A Distributed Execution Framework for AI | SciPy 2018] [Ray: A Cluster Computing Engine for Reinforcement Learning Applications | Spark Summit] [RLlib: Ray Reinforcement Learning Library | RISECamp 2018] [Enabling Composition in Distributed Reinforcement Learning | Spark Summit 2018] [Tune: Distributed Hyperparameter Search | RISECamp 2018]","title":"Talks (Videos)"},{"location":"overview-ray/more-information/#slides","text":"Talk given at UC Berkeley DS100 Talk given in October 2019 [Tune] Talk given at RISECamp 2019","title":"Slides"},{"location":"overview-ray/more-information/#papers","text":"Ray 1.0 Architecture whitepaper (new) Ray Design Patterns (new) RLlib paper RLlib flow paper Tune paper Older papers: Ray paper Ray HotOS paper","title":"Papers"},{"location":"overview-ray/what-and-why-ray/","text":"Overview Today\u2019s machine learning workloads like deep learning and hyperparameter tuning are compute-intensive. They require distributed or parallel execution at scale. As more and more vertical industries incorporate the state-of-the-art (SOTA) ML applications, the current compute-strata does not meet applications\u2019 demands. And these ML applications need more and more data for training, demanding further need of distributed data at low latency and high throughput. Therefore, the choice is nothing but to distribute compute at a massive scale to meet these applications\u2019 compute and IO demands. In other words, your computing demands are elastic: they scale from a single node or laptop to a massive parallel compute and data infrastructure. Ray makes it easy to parallelize single machine code \u2014 go from a single CPU to multi-core, multi-GPU, or multi-node with minimal code changes. What and Why Ray? Ray is an open-source project developed at UC Berkeley RISE Lab. As a general-purpose and universal distributed compute framework, it allows any compute-intensive Python workload \u2014 from distributed training or hyperparameter tuning to deep learning training and production model serving. It provides a set of distributed programming primitives\u2014tasks, actors, and object\u2014APIs in Python, C++, and Java to developers. The Ray autoscaler and the Ray runtime handle the scheduling, distributing, and fault-tolerance needs of your application. Although most developers will use its Python API bindings, Python on its own is limited to a single threaded programming model, because of the infamous global interpreter lock (GIL). It's limited in its capabilities in taking advantage of multiple cores on the single machine. However, its multiprocessing modules comes to the rescue on a single host. But beyond a single node, you will need a distributed framework like Ray. With a rich set of ML libraries and integrations built on a flexible distributed execution framework, Ray makes distributed computing easy and accessible to every engineer. Because most popular ML libraries have Python bindings, data scientists and ML developers are attracted to Ray for three reasons. First, writing distributed computing is easy and intuitive. You don\u2019t have to understand all the communication and scheduling nuances and you don\u2019t have to reason about it. With Ray\u2019s simple primitives, you can take any Python function or class and convert it into its distributed setting: just add a decorator, and you are done. Ray\u2019s distributed primitives APIs are simple, with all the complexity handled by Ray\u2019s execution framework. The functions will be scheduled for executing as stateless tasks, whereas the class will be a stateful remote service. Second, most popular ML libraries have strong integrations with Ray, and Ray\u2019s native libraries incorporate them. Composability is its strength. For example, you can use XGBBoost easily with Ray Train just as easily as you can use HuggingFace easily with Ray Serve. Or you can use PyTorch and TensorFlow just as easily with Ray Train. In short, it has a rich ecosystem of integrations with not only ML libraries but also other tools and frameworks. And third, you can use your laptop, as most developers invariably do, for development. When you want to scale or extend it to a Ray cluster, you can easily do so with only a single line or no line change of code. RAY_ADDRESS=ray://<cluster>:<port> python your_script.py Is Ray For You? If you are building distributed applications You want to take advantage of the PyData ecosystem with Ray\u2019s capabilities for distributed computing. If you are building machine learning solutions You want to use the latest and greatest ML frameworks locally for development in an IDE or Jupyter notebook. You want to experiment and iterate over your machine learning models on your laptop with a small set of data and then scale it on a cluster, with multiple nodes and multiple cores of hardware accelerators, with no or as few lines of code as possible and not worry about library management and dependencies on the remote cluster. You want to build deep learning models, do distribute training with PyTorch or TensorFlow using their parallel distributed training algorithms and strategies. You want to conduct model inference at a massive scale with a large batch of data, and do it in a distributed fashion. You want to deploy your ML models at scale and adhere to the latest ML serving design patterns. You want to take advantage of the PyData ecosystem with Ray\u2019s capabilities for distributed computing. If you are deploying Ray on your infrastructure You want to conduct model inference at a massive scale with a large batch of data, and do it in a distributed fashion. You want to deploy your ML models at scale and adhere to the latest ML serving design patterns. All or a few of the above are good reasons why Ray is for you.","title":"What and why Ray"},{"location":"overview-ray/what-and-why-ray/#overview","text":"Today\u2019s machine learning workloads like deep learning and hyperparameter tuning are compute-intensive. They require distributed or parallel execution at scale. As more and more vertical industries incorporate the state-of-the-art (SOTA) ML applications, the current compute-strata does not meet applications\u2019 demands. And these ML applications need more and more data for training, demanding further need of distributed data at low latency and high throughput. Therefore, the choice is nothing but to distribute compute at a massive scale to meet these applications\u2019 compute and IO demands. In other words, your computing demands are elastic: they scale from a single node or laptop to a massive parallel compute and data infrastructure. Ray makes it easy to parallelize single machine code \u2014 go from a single CPU to multi-core, multi-GPU, or multi-node with minimal code changes.","title":"Overview"},{"location":"overview-ray/what-and-why-ray/#what-and-why-ray","text":"Ray is an open-source project developed at UC Berkeley RISE Lab. As a general-purpose and universal distributed compute framework, it allows any compute-intensive Python workload \u2014 from distributed training or hyperparameter tuning to deep learning training and production model serving. It provides a set of distributed programming primitives\u2014tasks, actors, and object\u2014APIs in Python, C++, and Java to developers. The Ray autoscaler and the Ray runtime handle the scheduling, distributing, and fault-tolerance needs of your application. Although most developers will use its Python API bindings, Python on its own is limited to a single threaded programming model, because of the infamous global interpreter lock (GIL). It's limited in its capabilities in taking advantage of multiple cores on the single machine. However, its multiprocessing modules comes to the rescue on a single host. But beyond a single node, you will need a distributed framework like Ray. With a rich set of ML libraries and integrations built on a flexible distributed execution framework, Ray makes distributed computing easy and accessible to every engineer. Because most popular ML libraries have Python bindings, data scientists and ML developers are attracted to Ray for three reasons. First, writing distributed computing is easy and intuitive. You don\u2019t have to understand all the communication and scheduling nuances and you don\u2019t have to reason about it. With Ray\u2019s simple primitives, you can take any Python function or class and convert it into its distributed setting: just add a decorator, and you are done. Ray\u2019s distributed primitives APIs are simple, with all the complexity handled by Ray\u2019s execution framework. The functions will be scheduled for executing as stateless tasks, whereas the class will be a stateful remote service. Second, most popular ML libraries have strong integrations with Ray, and Ray\u2019s native libraries incorporate them. Composability is its strength. For example, you can use XGBBoost easily with Ray Train just as easily as you can use HuggingFace easily with Ray Serve. Or you can use PyTorch and TensorFlow just as easily with Ray Train. In short, it has a rich ecosystem of integrations with not only ML libraries but also other tools and frameworks. And third, you can use your laptop, as most developers invariably do, for development. When you want to scale or extend it to a Ray cluster, you can easily do so with only a single line or no line change of code. RAY_ADDRESS=ray://<cluster>:<port> python your_script.py","title":"What and Why Ray?"},{"location":"overview-ray/what-and-why-ray/#is-ray-for-you","text":"","title":"Is Ray For You?"},{"location":"overview-ray/what-and-why-ray/#if-you-are-building-distributed-applications","text":"You want to take advantage of the PyData ecosystem with Ray\u2019s capabilities for distributed computing.","title":"If you are building distributed applications"},{"location":"overview-ray/what-and-why-ray/#if-you-are-building-machine-learning-solutions","text":"You want to use the latest and greatest ML frameworks locally for development in an IDE or Jupyter notebook. You want to experiment and iterate over your machine learning models on your laptop with a small set of data and then scale it on a cluster, with multiple nodes and multiple cores of hardware accelerators, with no or as few lines of code as possible and not worry about library management and dependencies on the remote cluster. You want to build deep learning models, do distribute training with PyTorch or TensorFlow using their parallel distributed training algorithms and strategies. You want to conduct model inference at a massive scale with a large batch of data, and do it in a distributed fashion. You want to deploy your ML models at scale and adhere to the latest ML serving design patterns. You want to take advantage of the PyData ecosystem with Ray\u2019s capabilities for distributed computing.","title":"If you are building machine learning solutions"},{"location":"overview-ray/what-and-why-ray/#if-you-are-deploying-ray-on-your-infrastructure","text":"You want to conduct model inference at a massive scale with a large batch of data, and do it in a distributed fashion. You want to deploy your ML models at scale and adhere to the latest ML serving design patterns. All or a few of the above are good reasons why Ray is for you.","title":"If you are deploying Ray on your infrastructure"},{"location":"ray-deployment/getting-started/","text":"getting started","title":"Deploy Ray"},{"location":"ray-deployment/key-concepts/","text":"key_concepts","title":"Key concepts"},{"location":"ray-deployment/overview/","text":"overview","title":"Overview"},{"location":"ray-deployment/tutorials/","text":"tutorials","title":"Tutorials"},{"location":"ray-deployment/installing-ray/building-ray-on-linux-and-macos/","text":"Building Ray on Linux and macOS Tip: If you are only editing Tune/RLlib/Autoscaler files, follow instructions for python-develop to avoid long build times. To build Ray, first install the necessary dependencies by running the following commands: For Ubuntu, run the following commands: Ubuntu RHELv8 (Redhat EL 8.0-64 Minimal) macOS sudo apt-get update sudo apt-get install -y build-essential curl unzip psmisc pip install cython == 0 .29.0 pytest sudo yum groupinstall 'Development Tools' sudo yum install psmisc pip install cython == 0 .29.0 pytest # Install bazel manually from link: # <https://docs.bazel.build/versions/main/install-redhat.html> brew update brew install wget pip install cython == 0 .29.0 pytest Tip: Assuming you already have brew and bazel installed on your mac and you also have grpc and protobuf installed on your mac consider removing those (grpc and protobuf) for smooth build through commands brew uninstall grpc , brew uninstall protobuf . If you have built the source code earlier and it still fails with error as No such file or directory: , try cleaning previous builds on your host by running commands brew uninstall binutils and bazel clean --expunge . Ray can be built from the repository as follows. git clone https://github.com/ray-project/ray.git # Install Bazel. ray/ci/travis/install-bazel.sh # (Windows users: please manually place Bazel in your PATH, and point # BAZEL_SH to MSYS2's Bash: ``set BAZEL_SH=C:\\Program Files\\Git\\bin\\bash.exe``) # Build the dashboard # (requires Node.js, see https://nodejs.org/ for more information). pushd ray/dashboard/client npm install npm run build popd # Install Ray. cd ray/python pip install -e . --verbose # Add --user if you see a permission denied error. The -e means \"editable\", so changes you make to files in the Ray directory will take effect without reinstalling the package. Warning: if you run python setup.py install , files will be copied from the Ray directory to a directory of Python packages ( /lib/python3.6/site-packages/ray ). This means that changes you make to files in the Ray directory will not have any effect.","title":"Building Ray on Linux and macOS"},{"location":"ray-deployment/installing-ray/building-ray-on-linux-and-macos/#building-ray-on-linux-and-macos","text":"Tip: If you are only editing Tune/RLlib/Autoscaler files, follow instructions for python-develop to avoid long build times. To build Ray, first install the necessary dependencies by running the following commands: For Ubuntu, run the following commands: Ubuntu RHELv8 (Redhat EL 8.0-64 Minimal) macOS sudo apt-get update sudo apt-get install -y build-essential curl unzip psmisc pip install cython == 0 .29.0 pytest sudo yum groupinstall 'Development Tools' sudo yum install psmisc pip install cython == 0 .29.0 pytest # Install bazel manually from link: # <https://docs.bazel.build/versions/main/install-redhat.html> brew update brew install wget pip install cython == 0 .29.0 pytest Tip: Assuming you already have brew and bazel installed on your mac and you also have grpc and protobuf installed on your mac consider removing those (grpc and protobuf) for smooth build through commands brew uninstall grpc , brew uninstall protobuf . If you have built the source code earlier and it still fails with error as No such file or directory: , try cleaning previous builds on your host by running commands brew uninstall binutils and bazel clean --expunge . Ray can be built from the repository as follows. git clone https://github.com/ray-project/ray.git # Install Bazel. ray/ci/travis/install-bazel.sh # (Windows users: please manually place Bazel in your PATH, and point # BAZEL_SH to MSYS2's Bash: ``set BAZEL_SH=C:\\Program Files\\Git\\bin\\bash.exe``) # Build the dashboard # (requires Node.js, see https://nodejs.org/ for more information). pushd ray/dashboard/client npm install npm run build popd # Install Ray. cd ray/python pip install -e . --verbose # Add --user if you see a permission denied error. The -e means \"editable\", so changes you make to files in the Ray directory will take effect without reinstalling the package. Warning: if you run python setup.py install , files will be copied from the Ray directory to a directory of Python packages ( /lib/python3.6/site-packages/ray ). This means that changes you make to files in the Ray directory will not have any effect.","title":"Building Ray on Linux and macOS"},{"location":"ray-deployment/installing-ray/building-ray-on-windows/","text":"Building Ray on Windows Requirements The following links were correct during the writing of this section. In case the URLs changed, search at the organizations' sites. bazel 4.2 ( https://github.com/bazelbuild/bazel/releases/tag/4.2.1 ) Microsoft Visual Studio 2019 (or Microsoft Build Tools 2019 - https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2019 ) JDK 15 ( https://www.oracle.com/java/technologies/javase-jdk15-downloads.html ) Miniconda 3 ( https://docs.conda.io/en/latest/miniconda.html ) git for Windows, version 2.31.1 or later ( https://git-scm.com/download/win ) Steps Enable Developer mode on Windows 10 systems. This is necessary so git can create symlinks. Open Settings app; Go to \"Update & Security\"; Go to \"For Developers\" on the left pane; Turn on \"Developer mode\". Add the following Miniconda subdirectories to PATH. If Miniconda was installed for all users, the following paths are correct. If Miniconda is installed for a single user, adjust the paths accordingly. C:\\ProgramData\\Miniconda3 C:\\ProgramData\\Miniconda3\\Scripts C:\\ProgramData\\Miniconda3\\Library\\bin Define an environment variable BAZEL_SH to point to bash.exe. If git for Windows was installed for all users, bash's path should be C:\\Program Files\\Git\\bin\\bash.exe . If git was installed for a single user, adjust the path accordingly. Bazel 4.2 installation. Go to bazel 4.2 release web page and download bazel-4.2.1-windows-x86_64.exe. Copy the exe into the directory of your choice. Define an environment variable BAZEL_PATH to full exe path (example: set BAZEL_PATH=C:\\bazel\\bazel.exe ). Also add the bazel directory to the PATH (example: set PATH=%PATH%;C:\\bazel ) Install cython and pytest: pip install cython == 0 .29.0 pytest Download ray source code and build it. # cd to the directory under which the ray source tree will be downloaded. git clone -c core.symlinks = true https://github.com/ray-project/ray.git cd ray \\p ython pip install -e . --verbose","title":"Building Ray on Windows"},{"location":"ray-deployment/installing-ray/building-ray-on-windows/#building-ray-on-windows","text":"","title":"Building Ray on Windows"},{"location":"ray-deployment/installing-ray/building-ray-on-windows/#requirements","text":"The following links were correct during the writing of this section. In case the URLs changed, search at the organizations' sites. bazel 4.2 ( https://github.com/bazelbuild/bazel/releases/tag/4.2.1 ) Microsoft Visual Studio 2019 (or Microsoft Build Tools 2019 - https://visualstudio.microsoft.com/downloads/#build-tools-for-visual-studio-2019 ) JDK 15 ( https://www.oracle.com/java/technologies/javase-jdk15-downloads.html ) Miniconda 3 ( https://docs.conda.io/en/latest/miniconda.html ) git for Windows, version 2.31.1 or later ( https://git-scm.com/download/win )","title":"Requirements"},{"location":"ray-deployment/installing-ray/building-ray-on-windows/#steps","text":"Enable Developer mode on Windows 10 systems. This is necessary so git can create symlinks. Open Settings app; Go to \"Update & Security\"; Go to \"For Developers\" on the left pane; Turn on \"Developer mode\". Add the following Miniconda subdirectories to PATH. If Miniconda was installed for all users, the following paths are correct. If Miniconda is installed for a single user, adjust the paths accordingly. C:\\ProgramData\\Miniconda3 C:\\ProgramData\\Miniconda3\\Scripts C:\\ProgramData\\Miniconda3\\Library\\bin Define an environment variable BAZEL_SH to point to bash.exe. If git for Windows was installed for all users, bash's path should be C:\\Program Files\\Git\\bin\\bash.exe . If git was installed for a single user, adjust the path accordingly. Bazel 4.2 installation. Go to bazel 4.2 release web page and download bazel-4.2.1-windows-x86_64.exe. Copy the exe into the directory of your choice. Define an environment variable BAZEL_PATH to full exe path (example: set BAZEL_PATH=C:\\bazel\\bazel.exe ). Also add the bazel directory to the PATH (example: set PATH=%PATH%;C:\\bazel ) Install cython and pytest: pip install cython == 0 .29.0 pytest Download ray source code and build it. # cd to the directory under which the ray source tree will be downloaded. git clone -c core.symlinks = true https://github.com/ray-project/ray.git cd ray \\p ython pip install -e . --verbose","title":"Steps"},{"location":"ray-distributed-compute/getting-started/","text":"Getting started with a 10-minute Ray core walk through This short walk through will quickly get you started with Ray core APIs. It will give you the feel for its simplicity and velocity with which you can quickly write a distributed application using its distributed primitives. More importantly this walk through will provide a preliminary introduction to basics concepts of Ray: Installing Ray Starting Ray Using remote functions (tasks)t Fetching values from object refs Specifying resources for tasks Objects in Ray (object refs) Using remote classes (actors) Installing Ray To run this walkthrough, install Ray with pip install -U ray . For the latest wheels (for a snapshot of master), you can use these instructions at Daily Releases (Nightlies . Starting Ray You can start Ray cluster with a single node or your laptop and utilize all its multiple cores. Python C++ Java import ray # Start Ray. If you're connecting to an existing cluster, you would use # ray.init(address=<cluster-address>) instead. ray . init () // Run `ray cpp --show-library-path` to find headers and libraries. #include <ray/api.h> int main ( int argc , char ** argv ) { // Start Ray runtime. If you're connecting to an existing cluster, you can set // the `RAY_ADDRESS` env var. ray :: Init (); ... } import io.ray.api.Ray ; public class MyRayApp { public static void main ( String [] args ) { // Start Ray runtime. If you're connecting to an existing cluster, you can set // the `-Dray.address=<cluster-address>` java system property. Ray . init (); ... } } Remote functions as Tasks Ray enables arbitrary functions to be executed asynchronously. These asynchronous Ray functions are called \u201cremote functions\u201d. They return immediately with a future reference, which can be fetched. Here is simple example. Python C++ Java # A regular Python function. def my_function (): return 1 # By adding the `@ray.remote` decorator, a regular Python function # becomes a Ray remote function. @ray . remote def my_function (): return 1 # To invoke this remote function, use the `remote` method. # This will immediately return an object ref (a future) and then create # a task that will be executed on a worker process. obj_ref = my_function . remote () @ray . remote def slow_function (): time . sleep ( 10 ) return 1 # Invocations of Ray remote functions happen in parallel. # All computation is performed in the background, driven by Ray's internal event loop. obj_refs = [] for _ in range ( 4 ): # This doesn't block. obj_ref = slow_function . remote () obj_refs . append ( obj_ref ) // A regular C++ function. int MyFunction () { return 1 ; } // Register as a remote function by `RAY_REMOTE`. RAY_REMOTE ( MyFunction ); // Invoke the above method as a Ray remote function. // This will immediately return an object ref (a future) and then create // a task that will be executed on a worker process. auto res = ray :: Task ( MyFunction ). Remote (); // The result can be retrieved with ``ray::ObjectRef::Get``. assert ( * res . Get () == 1 ); int SlowFunction () { std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 10 )); return 1 ; } RAY_REMOTE ( SlowFunction ); // Invocations of Ray remote functions happen in parallel. // All computation is performed in the background, driven by Ray's internal event loop. for ( int i = 0 ; i < 4 ; i ++ ) { // This doesn't block. ray :: Task ( SlowFunction ). Remote (); } public class MyRayApp { // A regular Java static method. public static int myFunction () { return 1 ; } } // Invoke the above method as a Ray remote function. // This will immediately return an object ref (a future) and then create // a task that will be executed on a worker process. ObjectRef < Integer > res = Ray . task ( MyRayApp :: myFunction ). remote (); // The result can be retrieved with ``ObjectRef::get``. Assert . assertTrue ( res . get () == 1 ); public class MyRayApp { public static int slowFunction () throws InterruptedException { TimeUnit . SECONDS . sleep ( 10 ); return 1 ; } } // Invocations of Ray remote functions happen in parallel. // All computation is performed in the background, driven by Ray's internal event loop. for ( int i = 0 ; i < 4 ; i ++ ) { // This doesn't block. Ray . task ( MyRayApp :: slowFunction ). remote (); } Fetching returned values From the above my_function , the result can be retrieved with ray.get , which is a blocking call. Python C++ Java assert ray . get ( obj_ref ) == 1 assert ( * obj_ref . Get () == 1 ); Assert . assertTrue ( objRef . get () == 1 ); You can also fetch list of objects returned, as we did above. Python C++ Java assert ray . get ( obj_refs ) == [ 1 , 1 , 1 , 1 ] auto results = ray :: Get ( obj_refs ); assert ( results . size () == 3 ); assert ( * results [ 0 ] == 1 ); assert ( * results [ 1 ] == 1 ); assert ( * results [ 2 ] == 1 ); List < Integer > results = Ray . get ( objectRefs ); Assert . assertEquals ( results , ImmutableList . of ( 1 , 1 , 1 )); This will return a list of [1, 1, 1, 1] Passing object refs to remote functions Object refs can also be passed into remote functions. When the function actually gets executed, on a remote host, the argument will be a retrieved in-line from an object store as a regular object . For example, take this function: Python C++ Java @ray . remote def function_with_an_argument ( value ): # argument in-line fetched or resolved as a value # no need to explicit ray.get(). Ray will handle resolving return value + 1 obj_ref1 = my_function . remote ( 0 ) assert ray . get ( obj_ref1 ) == 1 # You can pass an object ref as an argument to another Ray remote function. obj_ref2 = function_with_an_argument . remote ( obj_ref1 ) assert ray . get ( obj_ref2 ) == 2 static int FunctionWithAnArgument ( int value ) { return value + 1 ; } RAY_REMOTE ( FunctionWithAnArgument ); auto obj_ref1 = ray :: Task ( MyFunction ). Remote ( 0 ); assert ( * obj_ref1 . Get () == 1 ); // You can pass an object ref as an argument to another Ray remote function. auto obj_ref2 = ray :: Task ( FunctionWithAnArgument ). Remote ( obj_ref1 ); assert ( * obj_ref2 . Get () == 2 ); public class MyRayApp { public static int functionWithAnArgument ( int value ) { return value + 1 ; } } ObjectRef < Integer > objRef1 = Ray . task ( MyRayApp :: myFunction ). remote ( 0 ); Assert . assertTrue ( objRef1 . get () == 1 ); // You can pass an object ref as an argument to another Ray remote function. ObjectRef < Integer > objRef2 = Ray . task ( MyRayApp :: functionWithAnArgument , objRef1 ). remote (); Assert . assertTrue ( objRef2 . get () == 2 ); A couple of salient Ray behavior to note here: The second task will not be executed until the first task has finished executing because the second task depends on the output of the first task. If the two tasks are scheduled on different machines, the output of the first task (the value corresponding to obj_ref1/objRef1 ) will be sent over the network to the machine where the second task is scheduled. Returning multiple values from Ray tasks Being true to Pythonic, Ray can return multiple object refs, by specifying in the remote decorator. The returned object_refs can be retrieved individually via ray.get(obj_ref) . Python @ray . remote ( num_returns = 3 ) def return_multiple (): return 1 , 2 , 3 a , b , c = return_multiple . remote () assert ray . get ( a ) == 1 assert ray . get ( b ) == 2 assert ray . get ( c ) == 3 Cancelling Ray tasks Often you may want to cancel a long-running task on a remote host. You don't know where the task is scheduled, but using returned obj_ref, you instruct Ray to locate the task and terminate it. Python @ray . remote def blocking_operation (): time . sleep ( 10e6 ) obj_ref = blocking_operation . remote () # Use the obj_ref to terminate the remote task ray . cancel ( obj_ref ) Fetching an obj_ref of a cancelled task raises and exception. Python from ray.exceptions import TaskCancelledError # In case you want to be cautious. try : ray . get ( obj_ref ) except TaskCancelledError : print ( \"Object reference was cancelled.\" ) Specifying resources for a Ray task For compute intensive Ray application, you may want to assign resources, such as number of CPUs or GPUs. Ray can then schedule the task on the node on the cluster with the required compute resources. Ray will automatically detect the available GPUs and CPUs on the machine. However, you can override this default behavior by passing in specific resources. Python C++ Java ray . init ( num_cpus = 8 , num_gpus = 4 ) RayConfig config ; config . num_cpus = 8 ; config . num_gpus = 4 ; ray :: Init ( config ); Set Java system property : - Dray . resources = CPU : 8 , GPU : 4 The resource requirements of a task have implications for the Ray\u2019s scheduling concurrency. In particular, the sum of the resource requirements of all concurrently executing tasks on a given node cannot exceed the node\u2019s total resources. For a specific Ray task, you can specify individual resources as well. Python C++ Java # Specify required resources. @ray . remote ( num_cpus = 4 , num_gpus = 2 ) def my_function (): return 1 // Specify required resources. ray :: Task ( MyFunction ). SetResource ( \"CPU\" , 1.0 ). SetResource ( \"GPU\" , 4.0 ). Remote (); // Specify required resources. Ray . task ( MyRayApp :: myFunction ). setResource ( \"CPU\" , 1.0 ). setResource ( \"GPU\" , 4.0 ). remote (); Of the eight CPUs and four GPUs requested, four CPUs and two GPUs will be allocated when this above task is scheduled for execution. Note : If you do not specify any resources, the default is 1 CPU resource and no other resources. If specifying CPUs, Ray does not enforce isolation (i.e., your task is expected to honor its request). If specifying GPUs, Ray does provide isolation in forms of visible devices (setting the environment variable CUDA_VISIBLE_DEVICES ), but it is the task\u2019s responsibility to actually use the GPUs (e.g., through a deep learning framework like TensorFlow or PyTorch). Objects in Ray In Ray, we can create and compute on objects. We refer to these objects as remote objects, and we use object refs to refer to them. Remote objects are stored in shared-memory object stores , and there is one object store per node in the cluster. In the cluster setting, we may not actually know which machine each object lives on. An object ref is essentially a unique ID that can be used to refer to a remote object. If you\u2019re familiar with futures , our object refs are conceptually similar. Object refs can be created in multiple ways: They are returned by remote function calls. They are returned by ray.put . For example: Python C++ Java y = 1 object_ref = ray . put ( y ) // Put an object in Ray's object store. int y = 1 ; ray :: ObjectRef < int > object_ref = ray :: Put ( y ); // Put an object in Ray's object store. int y = 1 ; ObjectRef < Integer > objectRef = Ray . put ( y ); Fetching Objects in Ray Ray provides primitives `ray.get(object_ref) to retrieve object refs values from the object store. As a developer, you need not worry where or on what node's object store the object ref is stored. Ray keeps all the bookkeeping and meta-data associated with it and know precisely how to fetch its associated value. If the current node\u2019s object store, where ray.get(object_ref) is being executed, does not contain the object, the object is downloaded from where it's stored in the cluster. Also, if the object is a numpy array or a collection of numpy arrays, the get call is zero-copy and returns arrays backed by shared object store memory. Otherwise, we deserialize the object data into a Python object. Python C++ Java # Get the value of one object ref. obj_ref = ray . put ( 1 ) assert ray . get ( obj_ref ) == 1 # Get the values of multiple object refs in parallel. assert ray . get ([ ray . put ( i ) for i in range ( 3 )]) == [ 0 , 1 , 2 ] # You can also set a timeout to return early from a ``get`` that's blocking for too long. from ray.exceptions import GetTimeoutError @ray . remote def long_running_function (): time . sleep ( 8 ) obj_ref = long_running_function . remote () try : ray . get ( obj_ref , timeout = 4 ) except GetTimeoutError : print ( \"`get` timed out.\" ) // Get the value of one object ref. ray :: ObjectRef < int > obj_ref = ray :: Put ( 1 ); assert ( * obj_ref . Get () == 1 ); // Get the values of multiple object refs in parallel. std :: vector < ray :: ObjectRef < int >> obj_refs ; for ( int i = 0 ; i < 3 ; i ++ ) { obj_refs . emplace_back ( ray :: Put ( i )); } auto results = ray :: Get ( obj_refs ); assert ( results . size () == 3 ); assert ( * results [ 0 ] == 0 ); assert ( * results [ 1 ] == 1 ); assert ( * results [ 2 ] == 2 ); // Get the value of one object ref. ObjectRef < Integer > objRef = Ray . put ( 1 ); Assert . assertTrue ( objRef . get () == 1 ); // Get the values of multiple object refs in parallel. List < ObjectRef < Integer >> objectRefs = new ArrayList <> (); for ( int i = 0 ; i < 3 ; i ++ ) { objectRefs . add ( Ray . put ( i )); } List < Integer > results = Ray . get ( objectRefs ); Assert . assertEquals ( results , ImmutableList . of ( 0 , 1 , 2 )); After launching a number of tasks, you may want to know which ones have finished executing. This can be done with ray.wait . One way is to fetch only the finished tasks returned. You can programmatically do as follows. Python @ray . remote def f (): time . sleep ( 1 ) return \"done\" # Execute `f()` in a comprehension, returning a list of object refs obj_refs = [ f . remote () for i in range ( 5 )])] # Iterate over the unfinished tasks while len ( obj_refs ) > 0 : return_n = 2 if len ( obj_refs ) > 1 else 1 # only return no more than two finished tasks. This may block of no tasks are finished yet ready_refs , remaining_refs = ray . wait ( obj_refs , num_returns = return_n , timeout = 10.0 ) # print the finished tasks. This get won't block if len ( ready_refs ) > 0 : print ( ray . get ( ready_refs )) # Update the remaining ones obj_refs = remaining_refs [ done , done ] [ done , done ] [ done ] Remote Classes as Ray Actors Actors extend the Ray API from a function as remote-stateless task to class as remote-stateful service. An actor is essentially a stateful worker; its class methods can be executed as remote-stateful tasks. Let's see an easy example. Python C++ Java @ray . remote class Counter : def __init__ ( self ): self . value = 0 def increment ( self , val = 1 ): self . value += val return self . value # Create an actor from this class. counter = Counter . remote () // A regular C++ class. class Counter { private : int value = 0 ; public : int Increment () { value += 1 ; return value ; } }; // Factory function of Counter class. static Counter * CreateCounter () { return new Counter (); }; RAY_REMOTE ( & Counter :: Increment , CreateCounter ); // Create an actor from this class. // `ray::Actor` takes a factory method that can produce // a `Counter` object. Here, we pass `Counter`'s factory function // as the argument. auto counter = ray :: Actor ( CreateCounter ). Remote (); // A regular Java class. public class Counter { private int value = 0 ; public int increment () { this . value += 1 ; return this . value ; } } // Create an actor from this class. // `Ray.actor` takes a factory method that can produce // a `Counter` object. Here, we pass `Counter`'s constructor // as the argument. ActorHandle < Counter > counter = Ray . actor ( Counter :: new ). remote (); Specifying required resources As with Ray tasks, you can allocate compute resources to a Ray actor Python C++ Java @ray . remote class Actor ( num_cpus = 2 , num_gpus = 0.5 ) pass // Specify required resources for an actor. ray :: Actor ( CreateCounter ). SetResource ( \"CPU\" , 2.0 ). SetResource ( \"GPU\" , 0.5 ). Remote (); // Specify required resources for an actor. Ray . actor ( Counter :: new ). setResource ( \"CPU\" , 2.0 ). setResource ( \"GPU\" , 0.5 ). remote (); Note : Yes, you can allocate a fraction of a compute resource Calling the actor methods We can interact with the actor by calling its methods with the remote operator. We can then call get on the object ref to retrieve the actual value. Python C++ Java assert ray . get ( counter . remote ()) == 1 # Send the value 2 as an argument to its method via the `remote` method assert ray . get ( counter . remote ( 2 ) == 3 / Call the actor . auto object_ref = counter . Task ( & Counter :: increment ). Remote (); assert ( * object_ref . Get () == 1 ); auto object_ref = counter . Task ( & Counter :: increment ). Remote ( 2 ); assert ( * object_ref . Get () == 3 ); / Call the actor . ObjectRef < Integer > objectRef = counter . task ( & Counter :: increment ). remote (); Assert . assertTrue ( objectRef . get () == 1 ); / Call the actor with another value ObjectRef < Integer > objectRef = counter . task ( & Counter :: increment ). remote ( 2 ); Assert . assertTrue ( objectRef . get () == 3 ); Observe the state, the current value of the instance variable, is maintained by the remote actor. Methods called on different actors can execute in parallel, and methods called on the same actor are executed serially in the order that they are called. Methods on the same actor will share state with one another, as shown below. Python # Create ten instances of actors wit Counter counters = [ Count . remote () for _ in range ( 10 )] # Increment each Counter once and get the results. These tasks all happen in parallel results = ray . get ([ c . increment . remote () for c in counters ]) print ( results ) # prints [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] # Increment the first Counter five times. These tasks are executed serially # and share state. results = ray . get ([ counters [ 0 ] . increment . remote () for _ in range ( 5 )]) print ( results ) # prints [2, 3, 4 , 5, 6] Now that you have take a 10-minute walk through Ray core APIs and how they work, this is a good time to comprehend some Ray concepts that were used here as well as Ray architecture.","title":"Getting started"},{"location":"ray-distributed-compute/getting-started/#getting-started-with-a-10-minute-ray-core-walk-through","text":"This short walk through will quickly get you started with Ray core APIs. It will give you the feel for its simplicity and velocity with which you can quickly write a distributed application using its distributed primitives. More importantly this walk through will provide a preliminary introduction to basics concepts of Ray: Installing Ray Starting Ray Using remote functions (tasks)t Fetching values from object refs Specifying resources for tasks Objects in Ray (object refs) Using remote classes (actors)","title":"Getting started with a 10-minute Ray core walk through"},{"location":"ray-distributed-compute/getting-started/#installing-ray","text":"To run this walkthrough, install Ray with pip install -U ray . For the latest wheels (for a snapshot of master), you can use these instructions at Daily Releases (Nightlies .","title":"Installing Ray"},{"location":"ray-distributed-compute/getting-started/#starting-ray","text":"You can start Ray cluster with a single node or your laptop and utilize all its multiple cores. Python C++ Java import ray # Start Ray. If you're connecting to an existing cluster, you would use # ray.init(address=<cluster-address>) instead. ray . init () // Run `ray cpp --show-library-path` to find headers and libraries. #include <ray/api.h> int main ( int argc , char ** argv ) { // Start Ray runtime. If you're connecting to an existing cluster, you can set // the `RAY_ADDRESS` env var. ray :: Init (); ... } import io.ray.api.Ray ; public class MyRayApp { public static void main ( String [] args ) { // Start Ray runtime. If you're connecting to an existing cluster, you can set // the `-Dray.address=<cluster-address>` java system property. Ray . init (); ... } }","title":"Starting Ray"},{"location":"ray-distributed-compute/getting-started/#remote-functions-as-tasks","text":"Ray enables arbitrary functions to be executed asynchronously. These asynchronous Ray functions are called \u201cremote functions\u201d. They return immediately with a future reference, which can be fetched. Here is simple example. Python C++ Java # A regular Python function. def my_function (): return 1 # By adding the `@ray.remote` decorator, a regular Python function # becomes a Ray remote function. @ray . remote def my_function (): return 1 # To invoke this remote function, use the `remote` method. # This will immediately return an object ref (a future) and then create # a task that will be executed on a worker process. obj_ref = my_function . remote () @ray . remote def slow_function (): time . sleep ( 10 ) return 1 # Invocations of Ray remote functions happen in parallel. # All computation is performed in the background, driven by Ray's internal event loop. obj_refs = [] for _ in range ( 4 ): # This doesn't block. obj_ref = slow_function . remote () obj_refs . append ( obj_ref ) // A regular C++ function. int MyFunction () { return 1 ; } // Register as a remote function by `RAY_REMOTE`. RAY_REMOTE ( MyFunction ); // Invoke the above method as a Ray remote function. // This will immediately return an object ref (a future) and then create // a task that will be executed on a worker process. auto res = ray :: Task ( MyFunction ). Remote (); // The result can be retrieved with ``ray::ObjectRef::Get``. assert ( * res . Get () == 1 ); int SlowFunction () { std :: this_thread :: sleep_for ( std :: chrono :: seconds ( 10 )); return 1 ; } RAY_REMOTE ( SlowFunction ); // Invocations of Ray remote functions happen in parallel. // All computation is performed in the background, driven by Ray's internal event loop. for ( int i = 0 ; i < 4 ; i ++ ) { // This doesn't block. ray :: Task ( SlowFunction ). Remote (); } public class MyRayApp { // A regular Java static method. public static int myFunction () { return 1 ; } } // Invoke the above method as a Ray remote function. // This will immediately return an object ref (a future) and then create // a task that will be executed on a worker process. ObjectRef < Integer > res = Ray . task ( MyRayApp :: myFunction ). remote (); // The result can be retrieved with ``ObjectRef::get``. Assert . assertTrue ( res . get () == 1 ); public class MyRayApp { public static int slowFunction () throws InterruptedException { TimeUnit . SECONDS . sleep ( 10 ); return 1 ; } } // Invocations of Ray remote functions happen in parallel. // All computation is performed in the background, driven by Ray's internal event loop. for ( int i = 0 ; i < 4 ; i ++ ) { // This doesn't block. Ray . task ( MyRayApp :: slowFunction ). remote (); }","title":"Remote functions as Tasks"},{"location":"ray-distributed-compute/getting-started/#fetching-returned-values","text":"From the above my_function , the result can be retrieved with ray.get , which is a blocking call. Python C++ Java assert ray . get ( obj_ref ) == 1 assert ( * obj_ref . Get () == 1 ); Assert . assertTrue ( objRef . get () == 1 ); You can also fetch list of objects returned, as we did above. Python C++ Java assert ray . get ( obj_refs ) == [ 1 , 1 , 1 , 1 ] auto results = ray :: Get ( obj_refs ); assert ( results . size () == 3 ); assert ( * results [ 0 ] == 1 ); assert ( * results [ 1 ] == 1 ); assert ( * results [ 2 ] == 1 ); List < Integer > results = Ray . get ( objectRefs ); Assert . assertEquals ( results , ImmutableList . of ( 1 , 1 , 1 )); This will return a list of [1, 1, 1, 1]","title":"Fetching returned values"},{"location":"ray-distributed-compute/getting-started/#passing-object-refs-to-remote-functions","text":"Object refs can also be passed into remote functions. When the function actually gets executed, on a remote host, the argument will be a retrieved in-line from an object store as a regular object . For example, take this function: Python C++ Java @ray . remote def function_with_an_argument ( value ): # argument in-line fetched or resolved as a value # no need to explicit ray.get(). Ray will handle resolving return value + 1 obj_ref1 = my_function . remote ( 0 ) assert ray . get ( obj_ref1 ) == 1 # You can pass an object ref as an argument to another Ray remote function. obj_ref2 = function_with_an_argument . remote ( obj_ref1 ) assert ray . get ( obj_ref2 ) == 2 static int FunctionWithAnArgument ( int value ) { return value + 1 ; } RAY_REMOTE ( FunctionWithAnArgument ); auto obj_ref1 = ray :: Task ( MyFunction ). Remote ( 0 ); assert ( * obj_ref1 . Get () == 1 ); // You can pass an object ref as an argument to another Ray remote function. auto obj_ref2 = ray :: Task ( FunctionWithAnArgument ). Remote ( obj_ref1 ); assert ( * obj_ref2 . Get () == 2 ); public class MyRayApp { public static int functionWithAnArgument ( int value ) { return value + 1 ; } } ObjectRef < Integer > objRef1 = Ray . task ( MyRayApp :: myFunction ). remote ( 0 ); Assert . assertTrue ( objRef1 . get () == 1 ); // You can pass an object ref as an argument to another Ray remote function. ObjectRef < Integer > objRef2 = Ray . task ( MyRayApp :: functionWithAnArgument , objRef1 ). remote (); Assert . assertTrue ( objRef2 . get () == 2 ); A couple of salient Ray behavior to note here: The second task will not be executed until the first task has finished executing because the second task depends on the output of the first task. If the two tasks are scheduled on different machines, the output of the first task (the value corresponding to obj_ref1/objRef1 ) will be sent over the network to the machine where the second task is scheduled.","title":"Passing object refs to remote functions"},{"location":"ray-distributed-compute/getting-started/#returning-multiple-values-from-ray-tasks","text":"Being true to Pythonic, Ray can return multiple object refs, by specifying in the remote decorator. The returned object_refs can be retrieved individually via ray.get(obj_ref) . Python @ray . remote ( num_returns = 3 ) def return_multiple (): return 1 , 2 , 3 a , b , c = return_multiple . remote () assert ray . get ( a ) == 1 assert ray . get ( b ) == 2 assert ray . get ( c ) == 3","title":"Returning multiple values from Ray tasks"},{"location":"ray-distributed-compute/getting-started/#cancelling-ray-tasks","text":"Often you may want to cancel a long-running task on a remote host. You don't know where the task is scheduled, but using returned obj_ref, you instruct Ray to locate the task and terminate it. Python @ray . remote def blocking_operation (): time . sleep ( 10e6 ) obj_ref = blocking_operation . remote () # Use the obj_ref to terminate the remote task ray . cancel ( obj_ref ) Fetching an obj_ref of a cancelled task raises and exception. Python from ray.exceptions import TaskCancelledError # In case you want to be cautious. try : ray . get ( obj_ref ) except TaskCancelledError : print ( \"Object reference was cancelled.\" )","title":"Cancelling Ray tasks"},{"location":"ray-distributed-compute/getting-started/#specifying-resources-for-a-ray-task","text":"For compute intensive Ray application, you may want to assign resources, such as number of CPUs or GPUs. Ray can then schedule the task on the node on the cluster with the required compute resources. Ray will automatically detect the available GPUs and CPUs on the machine. However, you can override this default behavior by passing in specific resources. Python C++ Java ray . init ( num_cpus = 8 , num_gpus = 4 ) RayConfig config ; config . num_cpus = 8 ; config . num_gpus = 4 ; ray :: Init ( config ); Set Java system property : - Dray . resources = CPU : 8 , GPU : 4 The resource requirements of a task have implications for the Ray\u2019s scheduling concurrency. In particular, the sum of the resource requirements of all concurrently executing tasks on a given node cannot exceed the node\u2019s total resources. For a specific Ray task, you can specify individual resources as well. Python C++ Java # Specify required resources. @ray . remote ( num_cpus = 4 , num_gpus = 2 ) def my_function (): return 1 // Specify required resources. ray :: Task ( MyFunction ). SetResource ( \"CPU\" , 1.0 ). SetResource ( \"GPU\" , 4.0 ). Remote (); // Specify required resources. Ray . task ( MyRayApp :: myFunction ). setResource ( \"CPU\" , 1.0 ). setResource ( \"GPU\" , 4.0 ). remote (); Of the eight CPUs and four GPUs requested, four CPUs and two GPUs will be allocated when this above task is scheduled for execution. Note : If you do not specify any resources, the default is 1 CPU resource and no other resources. If specifying CPUs, Ray does not enforce isolation (i.e., your task is expected to honor its request). If specifying GPUs, Ray does provide isolation in forms of visible devices (setting the environment variable CUDA_VISIBLE_DEVICES ), but it is the task\u2019s responsibility to actually use the GPUs (e.g., through a deep learning framework like TensorFlow or PyTorch).","title":"Specifying resources for a Ray task"},{"location":"ray-distributed-compute/getting-started/#objects-in-ray","text":"In Ray, we can create and compute on objects. We refer to these objects as remote objects, and we use object refs to refer to them. Remote objects are stored in shared-memory object stores , and there is one object store per node in the cluster. In the cluster setting, we may not actually know which machine each object lives on. An object ref is essentially a unique ID that can be used to refer to a remote object. If you\u2019re familiar with futures , our object refs are conceptually similar. Object refs can be created in multiple ways: They are returned by remote function calls. They are returned by ray.put . For example: Python C++ Java y = 1 object_ref = ray . put ( y ) // Put an object in Ray's object store. int y = 1 ; ray :: ObjectRef < int > object_ref = ray :: Put ( y ); // Put an object in Ray's object store. int y = 1 ; ObjectRef < Integer > objectRef = Ray . put ( y );","title":"Objects in Ray"},{"location":"ray-distributed-compute/getting-started/#fetching-objects-in-ray","text":"Ray provides primitives `ray.get(object_ref) to retrieve object refs values from the object store. As a developer, you need not worry where or on what node's object store the object ref is stored. Ray keeps all the bookkeeping and meta-data associated with it and know precisely how to fetch its associated value. If the current node\u2019s object store, where ray.get(object_ref) is being executed, does not contain the object, the object is downloaded from where it's stored in the cluster. Also, if the object is a numpy array or a collection of numpy arrays, the get call is zero-copy and returns arrays backed by shared object store memory. Otherwise, we deserialize the object data into a Python object. Python C++ Java # Get the value of one object ref. obj_ref = ray . put ( 1 ) assert ray . get ( obj_ref ) == 1 # Get the values of multiple object refs in parallel. assert ray . get ([ ray . put ( i ) for i in range ( 3 )]) == [ 0 , 1 , 2 ] # You can also set a timeout to return early from a ``get`` that's blocking for too long. from ray.exceptions import GetTimeoutError @ray . remote def long_running_function (): time . sleep ( 8 ) obj_ref = long_running_function . remote () try : ray . get ( obj_ref , timeout = 4 ) except GetTimeoutError : print ( \"`get` timed out.\" ) // Get the value of one object ref. ray :: ObjectRef < int > obj_ref = ray :: Put ( 1 ); assert ( * obj_ref . Get () == 1 ); // Get the values of multiple object refs in parallel. std :: vector < ray :: ObjectRef < int >> obj_refs ; for ( int i = 0 ; i < 3 ; i ++ ) { obj_refs . emplace_back ( ray :: Put ( i )); } auto results = ray :: Get ( obj_refs ); assert ( results . size () == 3 ); assert ( * results [ 0 ] == 0 ); assert ( * results [ 1 ] == 1 ); assert ( * results [ 2 ] == 2 ); // Get the value of one object ref. ObjectRef < Integer > objRef = Ray . put ( 1 ); Assert . assertTrue ( objRef . get () == 1 ); // Get the values of multiple object refs in parallel. List < ObjectRef < Integer >> objectRefs = new ArrayList <> (); for ( int i = 0 ; i < 3 ; i ++ ) { objectRefs . add ( Ray . put ( i )); } List < Integer > results = Ray . get ( objectRefs ); Assert . assertEquals ( results , ImmutableList . of ( 0 , 1 , 2 )); After launching a number of tasks, you may want to know which ones have finished executing. This can be done with ray.wait . One way is to fetch only the finished tasks returned. You can programmatically do as follows. Python @ray . remote def f (): time . sleep ( 1 ) return \"done\" # Execute `f()` in a comprehension, returning a list of object refs obj_refs = [ f . remote () for i in range ( 5 )])] # Iterate over the unfinished tasks while len ( obj_refs ) > 0 : return_n = 2 if len ( obj_refs ) > 1 else 1 # only return no more than two finished tasks. This may block of no tasks are finished yet ready_refs , remaining_refs = ray . wait ( obj_refs , num_returns = return_n , timeout = 10.0 ) # print the finished tasks. This get won't block if len ( ready_refs ) > 0 : print ( ray . get ( ready_refs )) # Update the remaining ones obj_refs = remaining_refs [ done , done ] [ done , done ] [ done ]","title":"Fetching Objects in Ray"},{"location":"ray-distributed-compute/getting-started/#remote-classes-as-ray-actors","text":"Actors extend the Ray API from a function as remote-stateless task to class as remote-stateful service. An actor is essentially a stateful worker; its class methods can be executed as remote-stateful tasks. Let's see an easy example. Python C++ Java @ray . remote class Counter : def __init__ ( self ): self . value = 0 def increment ( self , val = 1 ): self . value += val return self . value # Create an actor from this class. counter = Counter . remote () // A regular C++ class. class Counter { private : int value = 0 ; public : int Increment () { value += 1 ; return value ; } }; // Factory function of Counter class. static Counter * CreateCounter () { return new Counter (); }; RAY_REMOTE ( & Counter :: Increment , CreateCounter ); // Create an actor from this class. // `ray::Actor` takes a factory method that can produce // a `Counter` object. Here, we pass `Counter`'s factory function // as the argument. auto counter = ray :: Actor ( CreateCounter ). Remote (); // A regular Java class. public class Counter { private int value = 0 ; public int increment () { this . value += 1 ; return this . value ; } } // Create an actor from this class. // `Ray.actor` takes a factory method that can produce // a `Counter` object. Here, we pass `Counter`'s constructor // as the argument. ActorHandle < Counter > counter = Ray . actor ( Counter :: new ). remote ();","title":"Remote Classes as Ray Actors"},{"location":"ray-distributed-compute/getting-started/#specifying-required-resources","text":"As with Ray tasks, you can allocate compute resources to a Ray actor Python C++ Java @ray . remote class Actor ( num_cpus = 2 , num_gpus = 0.5 ) pass // Specify required resources for an actor. ray :: Actor ( CreateCounter ). SetResource ( \"CPU\" , 2.0 ). SetResource ( \"GPU\" , 0.5 ). Remote (); // Specify required resources for an actor. Ray . actor ( Counter :: new ). setResource ( \"CPU\" , 2.0 ). setResource ( \"GPU\" , 0.5 ). remote (); Note : Yes, you can allocate a fraction of a compute resource","title":"Specifying required resources"},{"location":"ray-distributed-compute/getting-started/#calling-the-actor-methods","text":"We can interact with the actor by calling its methods with the remote operator. We can then call get on the object ref to retrieve the actual value. Python C++ Java assert ray . get ( counter . remote ()) == 1 # Send the value 2 as an argument to its method via the `remote` method assert ray . get ( counter . remote ( 2 ) == 3 / Call the actor . auto object_ref = counter . Task ( & Counter :: increment ). Remote (); assert ( * object_ref . Get () == 1 ); auto object_ref = counter . Task ( & Counter :: increment ). Remote ( 2 ); assert ( * object_ref . Get () == 3 ); / Call the actor . ObjectRef < Integer > objectRef = counter . task ( & Counter :: increment ). remote (); Assert . assertTrue ( objectRef . get () == 1 ); / Call the actor with another value ObjectRef < Integer > objectRef = counter . task ( & Counter :: increment ). remote ( 2 ); Assert . assertTrue ( objectRef . get () == 3 ); Observe the state, the current value of the instance variable, is maintained by the remote actor. Methods called on different actors can execute in parallel, and methods called on the same actor are executed serially in the order that they are called. Methods on the same actor will share state with one another, as shown below. Python # Create ten instances of actors wit Counter counters = [ Count . remote () for _ in range ( 10 )] # Increment each Counter once and get the results. These tasks all happen in parallel results = ray . get ([ c . increment . remote () for c in counters ]) print ( results ) # prints [1, 1, 1, 1, 1, 1, 1, 1, 1, 1] # Increment the first Counter five times. These tasks are executed serially # and share state. results = ray . get ([ counters [ 0 ] . increment . remote () for _ in range ( 5 )]) print ( results ) # prints [2, 3, 4 , 5, 6] Now that you have take a 10-minute walk through Ray core APIs and how they work, this is a good time to comprehend some Ray concepts that were used here as well as Ray architecture.","title":"Calling the actor methods"},{"location":"ray-distributed-compute/key-concepts/","text":"Key concepts and terms Like any new framework, especially a distributed one, Ray has many concepts and terms you will want to get familiar as you dive deeper in its user guide, understand terms used in getting started with Ray Ray cluster Head node Any worker node can be a head node, but Ray designates one of worker as a head node as the one on which a driver runs. In addition, a head node also is runs the global control store. Global Control Store (GCS) There is only a single instance of GCS, which only runs as process on the head node. It's a key-value store that contains system-level metadata, such as the locations of objects and actors, etc. Worker node A Ray instance consists of one or more worker nodes, each of which consists of the following physical processes: Raylet : A raylet is shared among all jobs on the same cluster. That is other nodes on can talk and communicate with it. In turn the raylet has two main components, run on separate threads: 1: Scheduler : It is responsible for resource management and fulfilling task arguments that are stored in the distributed object store. The individual schedulers in a cluster comprise the Ray distributed scheduler 2: Object store : A shared-memory object store (also known as the Plasma Object Store). It is responsible for storing and transferring large objects. The individual object stores in a cluster comprise the Ray distributed object store. Ray core basics Ray Application A program including a Ray script that calls ray.init() and uses Ray programing primitives such as tasks , actors , and Ray APIs such as ray.get() , ray.put() , and ray.wait() which can be executed locally or remotely on a Ray cluster. Also called the driver , this script can be executed anywhere on the cluster or locally and connected to the cluster, via ray.init(address=ray:URI) . Ray Job It is a collection of tasks, objects, and actors originating (recursively) from the same driver . Tasks (Remote functions) Tasks (remote functions) are asynchronous Ray functions. Ray enables arbitrary functions to be executed asynchronously on a cluster as show in the diagram. Any arbitrary Python function can be converted into a remote Ray task by decorating it with the ray.remote() decorator. import ray @ray.remote @ray.remote def f(x): return x * x Actors (Remote Classes) An actor is essentially a stateful worker (or a service). When a new actor is instantiated, a new worker process is created on the Ray cluster node where the actor is bound to. Methods of the actor are scheduled on that specific worker process on that node in the cluster. This worker process and can access and mutate the state of that actor service. import ray ray.init() @ray.remote class Counter(): def __init__(self): Self.value = 0 def increment(self, val=1) self.value += val return self.value counter = Counter.remote() object_ref = counter.increment.remote() assert ray.get(object_ref) == 1` object_ref2 = counter.increment.remote(5) assert ray.get(object_ref2) == 6 Remote objects In Ray, we can create and compute on objects. We refer to these objects as remote objects. Remote objects are stored on the node that creates an object or returns an object from a task. For example, val = 10 * 10 object_ref = ray.put(val) If this code is executed in the driver then the object_ref will be stored or located on the node where the driver is run. In contrast, if this code is executed in a remote task in a function, then the node on which the task was executed owns the object ref. @ray.remote def f(): val = 10 * 10 return val Object refs We use object refs to refer to remote objects. They are unique. import ray val = 10 * 10 object_ref = ray.put(val) print(object_ref) ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000001000000) obj_ref = f.remote() print(obj_ref) ObjectRef(a67dc375e60ddd1affffffffffffffffffffffff0100000001000000) The value of an object can be resolved using an ObjectRef . The ObjectRef comprises two fields: 1. A unique 20-byte identifier. This is a concatenation of the ID of the task that produced the object and the integer number of objects created by that task so far. 2. The address of the object\u2019s owner (a worker process). This consists of the worker process\u2019s unique ID, IP address and port, and local Raylet\u2019s unique ID. Dependencies or Environment Anything outside of the Ray script that your application or job needs to run, including files, packages, and environment variables. Files Code files, data files or other files that your Ray application needs to run. Packages External libraries or executables required by your Ray application, often installed via pip or conda. Runtime Environments A runtime environment describes the dependencies your Ray application needs to run, including files, packages, environment variables, and more. It is installed dynamically on the cluster at runtime. Running Ray Ray runtime Ray programs are able to parallelize and distribute by leveraging an underlying Ray runtime. The Ray runtime consists of multiple services/processes started in the background for communication, data transfer, scheduling, and more. The Ray runtime can be started on a laptop, a single server, or multiple servers. Ray client The Ray Client is an API that connects a Python script to a remote Ray cluster. Effectively, it allows you to leverage a remote Ray cluster just like you would with Ray running on your local machine. Local machine and Cluster The recommended way to connect to a remote Ray cluster is to use Ray Client, and we will call the machine running Ray Client your local machine. Driver An entry point of Ray applications that calls ray.init(address=\u2019auto\u2019) or ray.init() Job A period of execution between connecting to a cluster with ray.init() and disconnecting by calling ray.shutdown() or exiting the Ray script. Namespace A namespace is a logical grouping of jobs and named actors. When an actor is named, its name must be unique within the namespace. For example, # Create an actor with a name counter = Counter.options(name=\"some_name\").remote() Placement groups Bundle A bundle is a collection of \u201cresources\u201d, i.e. {\u201cGPU\u201d: 4}. A bundle must be able to fit on a single node on the Ray cluster. Bundles are then placed according to the \u201cplacement group strategy\u201d across nodes on the cluster. Placement group A placement group is a collection of bundles. Placement group strategy A placement group strategy is an algorithm for selecting nodes for bundle placement. Read more about placement strategies. Advanced concepts Serialization and Deserialization serialization or serialization is the process of translating a data structure or object state into a format that can be stored or transmitted and reconstructed later. The opposite operation, extracting a data structure from a series of bytes, is deserialization. Memory Ray system memory: memory used internally by Ray Redis memory used for storing the list of nodes and actors present in the cluster. The amount of memory used for these purposes is typically quite small. Raylet memory used by the C++ raylet process running on each node. This cannot be controlled, but is typically quite small. Application memory: memory used by your application Worker heap memory used by your application (e.g., in Python code or TensorFlow), best measured as the resident set size (RSS) of your application minus its shared memory usage (SHR) in commands such as top. The reason you need to subtract SHR is that object store shared memory is reported by the OS as shared with each worker. Not subtracting SHR will result in double counting memory usage. Object store memory Memory used when your application creates objects in the object store via ray.put and when returning values from remote functions. Objects are reference counted and evicted when they fall out of scope. There is an object store server running on each node. In Ray 1.3+, objects will be spilled to disk if the object store fills up. Object store shared memory Memory used when your application reads objects via ray.get . Note that if an object is already present on the node, this does not cause additional allocations. This allows large objects to be efficiently shared among many actors and tasks.","title":"Key concepts"},{"location":"ray-distributed-compute/key-concepts/#key-concepts-and-terms","text":"Like any new framework, especially a distributed one, Ray has many concepts and terms you will want to get familiar as you dive deeper in its user guide, understand terms used in getting started with Ray","title":"Key concepts and terms"},{"location":"ray-distributed-compute/key-concepts/#ray-cluster","text":"Head node Any worker node can be a head node, but Ray designates one of worker as a head node as the one on which a driver runs. In addition, a head node also is runs the global control store. Global Control Store (GCS) There is only a single instance of GCS, which only runs as process on the head node. It's a key-value store that contains system-level metadata, such as the locations of objects and actors, etc. Worker node A Ray instance consists of one or more worker nodes, each of which consists of the following physical processes: Raylet : A raylet is shared among all jobs on the same cluster. That is other nodes on can talk and communicate with it. In turn the raylet has two main components, run on separate threads: 1: Scheduler : It is responsible for resource management and fulfilling task arguments that are stored in the distributed object store. The individual schedulers in a cluster comprise the Ray distributed scheduler 2: Object store : A shared-memory object store (also known as the Plasma Object Store). It is responsible for storing and transferring large objects. The individual object stores in a cluster comprise the Ray distributed object store.","title":"Ray cluster"},{"location":"ray-distributed-compute/key-concepts/#ray-core-basics","text":"Ray Application A program including a Ray script that calls ray.init() and uses Ray programing primitives such as tasks , actors , and Ray APIs such as ray.get() , ray.put() , and ray.wait() which can be executed locally or remotely on a Ray cluster. Also called the driver , this script can be executed anywhere on the cluster or locally and connected to the cluster, via ray.init(address=ray:URI) . Ray Job It is a collection of tasks, objects, and actors originating (recursively) from the same driver . Tasks (Remote functions) Tasks (remote functions) are asynchronous Ray functions. Ray enables arbitrary functions to be executed asynchronously on a cluster as show in the diagram. Any arbitrary Python function can be converted into a remote Ray task by decorating it with the ray.remote() decorator. import ray @ray.remote @ray.remote def f(x): return x * x Actors (Remote Classes) An actor is essentially a stateful worker (or a service). When a new actor is instantiated, a new worker process is created on the Ray cluster node where the actor is bound to. Methods of the actor are scheduled on that specific worker process on that node in the cluster. This worker process and can access and mutate the state of that actor service. import ray ray.init() @ray.remote class Counter(): def __init__(self): Self.value = 0 def increment(self, val=1) self.value += val return self.value counter = Counter.remote() object_ref = counter.increment.remote() assert ray.get(object_ref) == 1` object_ref2 = counter.increment.remote(5) assert ray.get(object_ref2) == 6 Remote objects In Ray, we can create and compute on objects. We refer to these objects as remote objects. Remote objects are stored on the node that creates an object or returns an object from a task. For example, val = 10 * 10 object_ref = ray.put(val) If this code is executed in the driver then the object_ref will be stored or located on the node where the driver is run. In contrast, if this code is executed in a remote task in a function, then the node on which the task was executed owns the object ref. @ray.remote def f(): val = 10 * 10 return val Object refs We use object refs to refer to remote objects. They are unique. import ray val = 10 * 10 object_ref = ray.put(val) print(object_ref) ObjectRef(ffffffffffffffffffffffffffffffffffffffff0100000001000000) obj_ref = f.remote() print(obj_ref) ObjectRef(a67dc375e60ddd1affffffffffffffffffffffff0100000001000000) The value of an object can be resolved using an ObjectRef . The ObjectRef comprises two fields: 1. A unique 20-byte identifier. This is a concatenation of the ID of the task that produced the object and the integer number of objects created by that task so far. 2. The address of the object\u2019s owner (a worker process). This consists of the worker process\u2019s unique ID, IP address and port, and local Raylet\u2019s unique ID. Dependencies or Environment Anything outside of the Ray script that your application or job needs to run, including files, packages, and environment variables. Files Code files, data files or other files that your Ray application needs to run. Packages External libraries or executables required by your Ray application, often installed via pip or conda. Runtime Environments A runtime environment describes the dependencies your Ray application needs to run, including files, packages, environment variables, and more. It is installed dynamically on the cluster at runtime.","title":"Ray core basics"},{"location":"ray-distributed-compute/key-concepts/#running-ray","text":"Ray runtime Ray programs are able to parallelize and distribute by leveraging an underlying Ray runtime. The Ray runtime consists of multiple services/processes started in the background for communication, data transfer, scheduling, and more. The Ray runtime can be started on a laptop, a single server, or multiple servers. Ray client The Ray Client is an API that connects a Python script to a remote Ray cluster. Effectively, it allows you to leverage a remote Ray cluster just like you would with Ray running on your local machine. Local machine and Cluster The recommended way to connect to a remote Ray cluster is to use Ray Client, and we will call the machine running Ray Client your local machine. Driver An entry point of Ray applications that calls ray.init(address=\u2019auto\u2019) or ray.init() Job A period of execution between connecting to a cluster with ray.init() and disconnecting by calling ray.shutdown() or exiting the Ray script. Namespace A namespace is a logical grouping of jobs and named actors. When an actor is named, its name must be unique within the namespace. For example, # Create an actor with a name counter = Counter.options(name=\"some_name\").remote()","title":"Running Ray"},{"location":"ray-distributed-compute/key-concepts/#placement-groups","text":"Bundle A bundle is a collection of \u201cresources\u201d, i.e. {\u201cGPU\u201d: 4}. A bundle must be able to fit on a single node on the Ray cluster. Bundles are then placed according to the \u201cplacement group strategy\u201d across nodes on the cluster. Placement group A placement group is a collection of bundles. Placement group strategy A placement group strategy is an algorithm for selecting nodes for bundle placement. Read more about placement strategies.","title":"Placement groups"},{"location":"ray-distributed-compute/key-concepts/#advanced-concepts","text":"Serialization and Deserialization serialization or serialization is the process of translating a data structure or object state into a format that can be stored or transmitted and reconstructed later. The opposite operation, extracting a data structure from a series of bytes, is deserialization.","title":"Advanced concepts"},{"location":"ray-distributed-compute/key-concepts/#memory","text":"","title":"Memory"},{"location":"ray-distributed-compute/key-concepts/#ray-system-memory-memory-used-internally-by-ray","text":"Redis memory used for storing the list of nodes and actors present in the cluster. The amount of memory used for these purposes is typically quite small. Raylet memory used by the C++ raylet process running on each node. This cannot be controlled, but is typically quite small.","title":"Ray system memory: memory used internally by Ray"},{"location":"ray-distributed-compute/key-concepts/#application-memory-memory-used-by-your-application","text":"Worker heap memory used by your application (e.g., in Python code or TensorFlow), best measured as the resident set size (RSS) of your application minus its shared memory usage (SHR) in commands such as top. The reason you need to subtract SHR is that object store shared memory is reported by the OS as shared with each worker. Not subtracting SHR will result in double counting memory usage. Object store memory Memory used when your application creates objects in the object store via ray.put and when returning values from remote functions. Objects are reference counted and evicted when they fall out of scope. There is an object store server running on each node. In Ray 1.3+, objects will be spilled to disk if the object store fills up. Object store shared memory Memory used when your application reads objects via ray.get . Note that if an object is already present on the node, this does not cause additional allocations. This allows large objects to be efficiently shared among many actors and tasks.","title":"Application memory: memory used by your application"},{"location":"ray-distributed-compute/tutorials/","text":"tutorials","title":"Tutorials"},{"location":"ray-infra/overview/","text":"overview","title":"Overview"},{"location":"ray-ml/overview/","text":"overview","title":"Overview"},{"location":"ray-ml/ray-data/getting-started/","text":"Serve: Scalable and Programmable Serving Getting Started Ray Serve is an easy-to-use scalable model serving library built on Ray. Ray Serve is: Framework-agnostic : Use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, Tensorflow, and Keras, to Scikit-learn models, to arbitrary Python business logic. Python-first : Configure your model serving declaratively in pure Python, without needing YAML or JSON configs. Since Ray Serve is built on Ray, it allows you to easily scale to many machines, both in your datacenter and in the cloud. Ray Serve can be used in two primary ways to deploy your models at scale: Have Python functions and classes automatically placed behind HTTP endpoints. Alternatively, call them from within your existing Python web server <serve-web-server-integration-tutorial> using the Python-native servehandle-api . Installation Ray Serve supports Python versions 3.6 through 3.8, with experimental support for Python 3.9. To install Ray Serve, run the following command: pip install \"ray[serve]\" Why Ray Serve? There are generally two ways of serving machine learning applications, both with serious limitations: you can use a traditional web server ---your own Flask app---or you can use a cloud-hosted solution. The first approach is easy to get started with, but it's hard to scale each component. The second approach requires vendor lock-in (SageMaker), framework-specific tooling (TFServing), and a general lack of flexibility. Ray Serve solves these problems by giving you a simple web server (and the ability to use your own <serve-web-server-integration-tutorial> ) while still handling the complex routing, scaling, and testing logic necessary for production deployments. Beyond scaling up your deployments with multiple replicas, Ray Serve also enables: serve-model-composition ---ability to flexibly compose multiple models and independently scale and update each. serve-batching ---built in request batching to help you meet your performance objectives. serve-cpus-gpus ---specify fractional resource requirements to fully saturate each of your GPUs with several models. For more on the motivation behind Ray Serve, check out these [meetup slides][] and this [blog post][]. When should I use Ray Serve? Ray Serve is a flexible tool that's easy to use for deploying, operating, and monitoring Python-based machine learning applications. Ray Serve excels when you want to mix business logic with ML models and scaling out in production is a necessity. This might be because of large-scale batch processing requirements or because you want to scale up a model pipeline consisting of many individual models with different performance properties. If you plan on running on multiple machines, Ray Serve will serve you well!","title":"Getting started"},{"location":"ray-ml/ray-data/getting-started/#serve-scalable-and-programmable-serving","text":"","title":"Serve: Scalable and Programmable Serving"},{"location":"ray-ml/ray-data/getting-started/#getting-started","text":"Ray Serve is an easy-to-use scalable model serving library built on Ray. Ray Serve is: Framework-agnostic : Use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, Tensorflow, and Keras, to Scikit-learn models, to arbitrary Python business logic. Python-first : Configure your model serving declaratively in pure Python, without needing YAML or JSON configs. Since Ray Serve is built on Ray, it allows you to easily scale to many machines, both in your datacenter and in the cloud. Ray Serve can be used in two primary ways to deploy your models at scale: Have Python functions and classes automatically placed behind HTTP endpoints. Alternatively, call them from within your existing Python web server <serve-web-server-integration-tutorial> using the Python-native servehandle-api .","title":"Getting Started"},{"location":"ray-ml/ray-data/getting-started/#installation","text":"Ray Serve supports Python versions 3.6 through 3.8, with experimental support for Python 3.9. To install Ray Serve, run the following command: pip install \"ray[serve]\"","title":"Installation"},{"location":"ray-ml/ray-data/getting-started/#why-ray-serve","text":"There are generally two ways of serving machine learning applications, both with serious limitations: you can use a traditional web server ---your own Flask app---or you can use a cloud-hosted solution. The first approach is easy to get started with, but it's hard to scale each component. The second approach requires vendor lock-in (SageMaker), framework-specific tooling (TFServing), and a general lack of flexibility. Ray Serve solves these problems by giving you a simple web server (and the ability to use your own <serve-web-server-integration-tutorial> ) while still handling the complex routing, scaling, and testing logic necessary for production deployments. Beyond scaling up your deployments with multiple replicas, Ray Serve also enables: serve-model-composition ---ability to flexibly compose multiple models and independently scale and update each. serve-batching ---built in request batching to help you meet your performance objectives. serve-cpus-gpus ---specify fractional resource requirements to fully saturate each of your GPUs with several models. For more on the motivation behind Ray Serve, check out these [meetup slides][] and this [blog post][].","title":"Why Ray Serve?"},{"location":"ray-ml/ray-data/getting-started/#when-should-i-use-ray-serve","text":"Ray Serve is a flexible tool that's easy to use for deploying, operating, and monitoring Python-based machine learning applications. Ray Serve excels when you want to mix business logic with ML models and scaling out in production is a necessity. This might be because of large-scale batch processing requirements or because you want to scale up a model pipeline consisting of many individual models with different performance properties. If you plan on running on multiple machines, Ray Serve will serve you well!","title":"When should I use Ray Serve?"},{"location":"ray-ml/ray-data/key-concepts/","text":"Key Concepts Deployments Deployments are the central concept in Ray Serve. They allow you to define and update your business logic or models that will handle incoming requests as well as how this is exposed over HTTP or in Python. A deployment is defined using @serve.deployment <ray.serve.api.deployment> on a Python class (or function for simple use cases). You can specify arguments to be passed to the constructor when you call Deployment.deploy() , shown below. @serve . deployment class MyFirstDeployment : # Take the message to return as an argument to the constructor. def __init__ ( self , msg ): self . msg = msg def __call__ ( self , request ): return self . msg def other_method ( self , arg ): return self . msg MyFirstDeployment . deploy ( \"Hello world!\" ) Deployments can be exposed in two ways: over HTTP or in Python via the servehandle-api . By default, HTTP requests will be forwarded to the __call__ method of the class (or the function) and a Starlette Request object will be the sole argument. You can also define a deployment that wraps a FastAPI app for more flexible handling of HTTP requests. See serve-fastapi-http for details. Replicas A deployment consists of a number of replicas , which are individual copies of the function or class that are started in separate Ray Actors (processes).","title":"Key concepts"},{"location":"ray-ml/ray-data/key-concepts/#key-concepts","text":"","title":"Key Concepts"},{"location":"ray-ml/ray-data/key-concepts/#deployments","text":"Deployments are the central concept in Ray Serve. They allow you to define and update your business logic or models that will handle incoming requests as well as how this is exposed over HTTP or in Python. A deployment is defined using @serve.deployment <ray.serve.api.deployment> on a Python class (or function for simple use cases). You can specify arguments to be passed to the constructor when you call Deployment.deploy() , shown below. @serve . deployment class MyFirstDeployment : # Take the message to return as an argument to the constructor. def __init__ ( self , msg ): self . msg = msg def __call__ ( self , request ): return self . msg def other_method ( self , arg ): return self . msg MyFirstDeployment . deploy ( \"Hello world!\" ) Deployments can be exposed in two ways: over HTTP or in Python via the servehandle-api . By default, HTTP requests will be forwarded to the __call__ method of the class (or the function) and a Starlette Request object will be the sole argument. You can also define a deployment that wraps a FastAPI app for more flexible handling of HTTP requests. See serve-fastapi-http for details.","title":"Deployments"},{"location":"ray-ml/ray-data/key-concepts/#replicas","text":"A deployment consists of a number of replicas , which are individual copies of the function or class that are started in separate Ray Actors (processes).","title":"Replicas"},{"location":"ray-ml/ray-data/tutorials/","text":"Tutorials Quickstart By the end of this tutorial, you will have learned the basics of Ray Serve and will be ready to pick and choose from the advanced topics in the sidebar. First, install Ray Serve and all of its dependencies by running the following command in your terminal: pip install \"ray[serve]\" Now we will write a Python script to serve a simple \"Counter\" class over HTTP. You may open an interactive Python terminal and copy in the lines below as we go. First, import Ray and Ray Serve: import ray from ray import serve Ray Serve runs on top of a Ray cluster, so the next step is to start a local Ray cluster: ray . init () Next, start the Ray Serve runtime: serve . start () Now we will define a simple Counter class. The goal is to serve this class behind an HTTP endpoint using Ray Serve. By default, Ray Serve offers a simple HTTP proxy that will send requests to the class' __call__ method. The argument to this method will be a Starlette Request object. @serve . deployment class Counter : def __init__ ( self ): self . count = 0 def __call__ ( self , request ): self . count += 1 return { \"count\" : self . count } Notice that we made this class into a Deployment with the @serve.deployment <ray.serve.api.deployment> decorator. This decorator is where we could set various configuration options such as the number of replicas, unique name of the deployment (it defaults to the class name), or the HTTP route prefix to expose the deployment at. See the Deployment API reference for more details. In order to deploy this, we simply need to call Counter.deploy() . Counter . deploy () Now that our deployment is up and running, let's test it out by making a query over HTTP. In your browser, simply visit http://127.0.0.1:8000/Counter , and you should see the output {\"count\": 1\"} . If you keep refreshing the page, the count should increase, as expected. Now let's say we want to update this deployment to add another method to decrement the counter. Here, because we want more flexible HTTP configuration we'll use Serve's FastAPI integration. For more information on this, please see serve-fastapi-http . from fastapi import FastAPI app = FastAPI () @serve . deployment @serve . ingress ( app ) class Counter : def __init__ ( self ): self . count = 0 @app . get ( \"/\" ) def get ( self ): return { \"count\" : self . count } @app . get ( \"/incr\" ) def incr ( self ): self . count += 1 return { \"count\" : self . count } @app . get ( \"/decr\" ) def decr ( self ): self . count -= 1 return { \"count\" : self . count } We've now redefined the Counter class to wrap a FastAPI application. This class is exposing three HTTP routes: /Counter will get the current count, /Counter/incr will increment the count, and /Counter/decr will decrement the count. To redeploy this updated version of the Counter , all we need to do is run Counter.deploy() again. Serve will perform a rolling update here to replace the existing replicas with the new version we defined. Counter . deploy () If we test out the HTTP endpoint again, we can see this in action. Note that the count has been reset to zero because the new version of Counter was deployed. > curl -X GET localhost:8000/Counter/ { \"count\" : 0 } > curl -X GET localhost:8000/Counter/incr { \"count\" : 1 } > curl -X GET localhost:8000/Counter/decr { \"count\" : 0 } Congratulations, you just built and ran your first Ray Serve application!","title":"Tutorials"},{"location":"ray-ml/ray-data/tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"ray-ml/ray-data/tutorials/#quickstart","text":"By the end of this tutorial, you will have learned the basics of Ray Serve and will be ready to pick and choose from the advanced topics in the sidebar. First, install Ray Serve and all of its dependencies by running the following command in your terminal: pip install \"ray[serve]\" Now we will write a Python script to serve a simple \"Counter\" class over HTTP. You may open an interactive Python terminal and copy in the lines below as we go. First, import Ray and Ray Serve: import ray from ray import serve Ray Serve runs on top of a Ray cluster, so the next step is to start a local Ray cluster: ray . init () Next, start the Ray Serve runtime: serve . start () Now we will define a simple Counter class. The goal is to serve this class behind an HTTP endpoint using Ray Serve. By default, Ray Serve offers a simple HTTP proxy that will send requests to the class' __call__ method. The argument to this method will be a Starlette Request object. @serve . deployment class Counter : def __init__ ( self ): self . count = 0 def __call__ ( self , request ): self . count += 1 return { \"count\" : self . count } Notice that we made this class into a Deployment with the @serve.deployment <ray.serve.api.deployment> decorator. This decorator is where we could set various configuration options such as the number of replicas, unique name of the deployment (it defaults to the class name), or the HTTP route prefix to expose the deployment at. See the Deployment API reference for more details. In order to deploy this, we simply need to call Counter.deploy() . Counter . deploy () Now that our deployment is up and running, let's test it out by making a query over HTTP. In your browser, simply visit http://127.0.0.1:8000/Counter , and you should see the output {\"count\": 1\"} . If you keep refreshing the page, the count should increase, as expected. Now let's say we want to update this deployment to add another method to decrement the counter. Here, because we want more flexible HTTP configuration we'll use Serve's FastAPI integration. For more information on this, please see serve-fastapi-http . from fastapi import FastAPI app = FastAPI () @serve . deployment @serve . ingress ( app ) class Counter : def __init__ ( self ): self . count = 0 @app . get ( \"/\" ) def get ( self ): return { \"count\" : self . count } @app . get ( \"/incr\" ) def incr ( self ): self . count += 1 return { \"count\" : self . count } @app . get ( \"/decr\" ) def decr ( self ): self . count -= 1 return { \"count\" : self . count } We've now redefined the Counter class to wrap a FastAPI application. This class is exposing three HTTP routes: /Counter will get the current count, /Counter/incr will increment the count, and /Counter/decr will decrement the count. To redeploy this updated version of the Counter , all we need to do is run Counter.deploy() again. Serve will perform a rolling update here to replace the existing replicas with the new version we defined. Counter . deploy () If we test out the HTTP endpoint again, we can see this in action. Note that the count has been reset to zero because the new version of Counter was deployed. > curl -X GET localhost:8000/Counter/ { \"count\" : 0 } > curl -X GET localhost:8000/Counter/incr { \"count\" : 1 } > curl -X GET localhost:8000/Counter/decr { \"count\" : 0 } Congratulations, you just built and ran your first Ray Serve application!","title":"Quickstart"},{"location":"ray-ml/ray-rllib/getting-started/","text":"getting started","title":"Getting started"},{"location":"ray-ml/ray-rllib/key-concepts/","text":"key_concepts","title":"Key concepts"},{"location":"ray-ml/ray-rllib/tutorials/","text":"tutorials","title":"Tutorials"},{"location":"ray-ml/ray-serve/getting-started/","text":"Serve: Scalable and Programmable Serving Getting Started Ray Serve is an easy-to-use scalable model serving library built on Ray. Ray Serve is: Framework-agnostic : Use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, Tensorflow, and Keras, to Scikit-learn models, to arbitrary Python business logic. Python-first : Configure your model serving declaratively in pure Python, without needing YAML or JSON configs. Since Ray Serve is built on Ray, it allows you to easily scale to many machines, both in your datacenter and in the cloud. Ray Serve can be used in two primary ways to deploy your models at scale: Have Python functions and classes automatically placed behind HTTP endpoints. Alternatively, call them from within your existing Python web server <serve-web-server-integration-tutorial> using the Python-native servehandle-api . Installation Ray Serve supports Python versions 3.6 through 3.8, with experimental support for Python 3.9. To install Ray Serve, run the following command: pip install \"ray[serve]\" Why Ray Serve? There are generally two ways of serving machine learning applications, both with serious limitations: you can use a traditional web server ---your own Flask app---or you can use a cloud-hosted solution. The first approach is easy to get started with, but it's hard to scale each component. The second approach requires vendor lock-in (SageMaker), framework-specific tooling (TFServing), and a general lack of flexibility. Ray Serve solves these problems by giving you a simple web server (and the ability to use your own <serve-web-server-integration-tutorial> ) while still handling the complex routing, scaling, and testing logic necessary for production deployments. Beyond scaling up your deployments with multiple replicas, Ray Serve also enables: serve-model-composition ---ability to flexibly compose multiple models and independently scale and update each. serve-batching ---built in request batching to help you meet your performance objectives. serve-cpus-gpus ---specify fractional resource requirements to fully saturate each of your GPUs with several models. For more on the motivation behind Ray Serve, check out these [meetup slides][] and this [blog post][]. When should I use Ray Serve? Ray Serve is a flexible tool that's easy to use for deploying, operating, and monitoring Python-based machine learning applications. Ray Serve excels when you want to mix business logic with ML models and scaling out in production is a necessity. This might be because of large-scale batch processing requirements or because you want to scale up a model pipeline consisting of many individual models with different performance properties. If you plan on running on multiple machines, Ray Serve will serve you well!","title":"Getting started"},{"location":"ray-ml/ray-serve/getting-started/#serve-scalable-and-programmable-serving","text":"","title":"Serve: Scalable and Programmable Serving"},{"location":"ray-ml/ray-serve/getting-started/#getting-started","text":"Ray Serve is an easy-to-use scalable model serving library built on Ray. Ray Serve is: Framework-agnostic : Use a single toolkit to serve everything from deep learning models built with frameworks like PyTorch, Tensorflow, and Keras, to Scikit-learn models, to arbitrary Python business logic. Python-first : Configure your model serving declaratively in pure Python, without needing YAML or JSON configs. Since Ray Serve is built on Ray, it allows you to easily scale to many machines, both in your datacenter and in the cloud. Ray Serve can be used in two primary ways to deploy your models at scale: Have Python functions and classes automatically placed behind HTTP endpoints. Alternatively, call them from within your existing Python web server <serve-web-server-integration-tutorial> using the Python-native servehandle-api .","title":"Getting Started"},{"location":"ray-ml/ray-serve/getting-started/#installation","text":"Ray Serve supports Python versions 3.6 through 3.8, with experimental support for Python 3.9. To install Ray Serve, run the following command: pip install \"ray[serve]\"","title":"Installation"},{"location":"ray-ml/ray-serve/getting-started/#why-ray-serve","text":"There are generally two ways of serving machine learning applications, both with serious limitations: you can use a traditional web server ---your own Flask app---or you can use a cloud-hosted solution. The first approach is easy to get started with, but it's hard to scale each component. The second approach requires vendor lock-in (SageMaker), framework-specific tooling (TFServing), and a general lack of flexibility. Ray Serve solves these problems by giving you a simple web server (and the ability to use your own <serve-web-server-integration-tutorial> ) while still handling the complex routing, scaling, and testing logic necessary for production deployments. Beyond scaling up your deployments with multiple replicas, Ray Serve also enables: serve-model-composition ---ability to flexibly compose multiple models and independently scale and update each. serve-batching ---built in request batching to help you meet your performance objectives. serve-cpus-gpus ---specify fractional resource requirements to fully saturate each of your GPUs with several models. For more on the motivation behind Ray Serve, check out these [meetup slides][] and this [blog post][].","title":"Why Ray Serve?"},{"location":"ray-ml/ray-serve/getting-started/#when-should-i-use-ray-serve","text":"Ray Serve is a flexible tool that's easy to use for deploying, operating, and monitoring Python-based machine learning applications. Ray Serve excels when you want to mix business logic with ML models and scaling out in production is a necessity. This might be because of large-scale batch processing requirements or because you want to scale up a model pipeline consisting of many individual models with different performance properties. If you plan on running on multiple machines, Ray Serve will serve you well!","title":"When should I use Ray Serve?"},{"location":"ray-ml/ray-serve/key-concepts/","text":"Key Concepts Deployments Deployments are the central concept in Ray Serve. They allow you to define and update your business logic or models that will handle incoming requests as well as how this is exposed over HTTP or in Python. A deployment is defined using @serve.deployment <ray.serve.api.deployment> on a Python class (or function for simple use cases). You can specify arguments to be passed to the constructor when you call Deployment.deploy() , shown below. @serve . deployment class MyFirstDeployment : # Take the message to return as an argument to the constructor. def __init__ ( self , msg ): self . msg = msg def __call__ ( self , request ): return self . msg def other_method ( self , arg ): return self . msg MyFirstDeployment . deploy ( \"Hello world!\" ) Deployments can be exposed in two ways: over HTTP or in Python via the servehandle-api . By default, HTTP requests will be forwarded to the __call__ method of the class (or the function) and a Starlette Request object will be the sole argument. You can also define a deployment that wraps a FastAPI app for more flexible handling of HTTP requests. See serve-fastapi-http for details. Replicas A deployment consists of a number of replicas , which are individual copies of the function or class that are started in separate Ray Actors (processes).","title":"Key concepts"},{"location":"ray-ml/ray-serve/key-concepts/#key-concepts","text":"","title":"Key Concepts"},{"location":"ray-ml/ray-serve/key-concepts/#deployments","text":"Deployments are the central concept in Ray Serve. They allow you to define and update your business logic or models that will handle incoming requests as well as how this is exposed over HTTP or in Python. A deployment is defined using @serve.deployment <ray.serve.api.deployment> on a Python class (or function for simple use cases). You can specify arguments to be passed to the constructor when you call Deployment.deploy() , shown below. @serve . deployment class MyFirstDeployment : # Take the message to return as an argument to the constructor. def __init__ ( self , msg ): self . msg = msg def __call__ ( self , request ): return self . msg def other_method ( self , arg ): return self . msg MyFirstDeployment . deploy ( \"Hello world!\" ) Deployments can be exposed in two ways: over HTTP or in Python via the servehandle-api . By default, HTTP requests will be forwarded to the __call__ method of the class (or the function) and a Starlette Request object will be the sole argument. You can also define a deployment that wraps a FastAPI app for more flexible handling of HTTP requests. See serve-fastapi-http for details.","title":"Deployments"},{"location":"ray-ml/ray-serve/key-concepts/#replicas","text":"A deployment consists of a number of replicas , which are individual copies of the function or class that are started in separate Ray Actors (processes).","title":"Replicas"},{"location":"ray-ml/ray-serve/tutorials/","text":"Tutorials Quickstart By the end of this tutorial, you will have learned the basics of Ray Serve and will be ready to pick and choose from the advanced topics in the sidebar. First, install Ray Serve and all of its dependencies by running the following command in your terminal: pip install \"ray[serve]\" Now we will write a Python script to serve a simple \"Counter\" class over HTTP. You may open an interactive Python terminal and copy in the lines below as we go. First, import Ray and Ray Serve: import ray from ray import serve Ray Serve runs on top of a Ray cluster, so the next step is to start a local Ray cluster: ray . init () Next, start the Ray Serve runtime: serve . start () Now we will define a simple Counter class. The goal is to serve this class behind an HTTP endpoint using Ray Serve. By default, Ray Serve offers a simple HTTP proxy that will send requests to the class' __call__ method. The argument to this method will be a Starlette Request object. @serve . deployment class Counter : def __init__ ( self ): self . count = 0 def __call__ ( self , request ): self . count += 1 return { \"count\" : self . count } Notice that we made this class into a Deployment with the @serve.deployment decorator. This decorator is where we could set various configuration options such as the number of replicas, unique name of the deployment (it defaults to the class name), or the HTTP route prefix to expose the deployment at. See the Deployment API reference for more details. In order to deploy this, we simply need to call Counter.deploy() . Counter . deploy () Now that our deployment is up and running, let's test it out by making a query over HTTP. In your browser, simply visit http://127.0.0.1:8000/Counter , and you should see the output {\"count\": 1\"} . If you keep refreshing the page, the count should increase, as expected. Now let's say we want to update this deployment to add another method to decrement the counter. Here, because we want more flexible HTTP configuration we'll use Serve's FastAPI integration. For more information on this, please see serve-fastapi-http . from fastapi import FastAPI app = FastAPI () @serve . deployment @serve . ingress ( app ) class Counter : def __init__ ( self ): self . count = 0 @app . get ( \"/\" ) def get ( self ): return { \"count\" : self . count } @app . get ( \"/incr\" ) def incr ( self ): self . count += 1 return { \"count\" : self . count } @app . get ( \"/decr\" ) def decr ( self ): self . count -= 1 return { \"count\" : self . count } We've now redefined the Counter class to wrap a FastAPI application. This class is exposing three HTTP routes: /Counter will get the current count, /Counter/incr will increment the count, and /Counter/decr will decrement the count. To redeploy this updated version of the Counter , all we need to do is run Counter.deploy() again. Serve will perform a rolling update here to replace the existing replicas with the new version we defined. Counter . deploy () If we test out the HTTP endpoint again, we can see this in action. Note that the count has been reset to zero because the new version of Counter was deployed. > curl -X GET localhost:8000/Counter/ { \"count\" : 0 } > curl -X GET localhost:8000/Counter/incr { \"count\" : 1 } > curl -X GET localhost:8000/Counter/decr { \"count\" : 0 } Congratulations, you just built and ran your first Ray Serve application!","title":"Model composition"},{"location":"ray-ml/ray-serve/tutorials/#tutorials","text":"","title":"Tutorials"},{"location":"ray-ml/ray-serve/tutorials/#quickstart","text":"By the end of this tutorial, you will have learned the basics of Ray Serve and will be ready to pick and choose from the advanced topics in the sidebar. First, install Ray Serve and all of its dependencies by running the following command in your terminal: pip install \"ray[serve]\" Now we will write a Python script to serve a simple \"Counter\" class over HTTP. You may open an interactive Python terminal and copy in the lines below as we go. First, import Ray and Ray Serve: import ray from ray import serve Ray Serve runs on top of a Ray cluster, so the next step is to start a local Ray cluster: ray . init () Next, start the Ray Serve runtime: serve . start () Now we will define a simple Counter class. The goal is to serve this class behind an HTTP endpoint using Ray Serve. By default, Ray Serve offers a simple HTTP proxy that will send requests to the class' __call__ method. The argument to this method will be a Starlette Request object. @serve . deployment class Counter : def __init__ ( self ): self . count = 0 def __call__ ( self , request ): self . count += 1 return { \"count\" : self . count } Notice that we made this class into a Deployment with the @serve.deployment decorator. This decorator is where we could set various configuration options such as the number of replicas, unique name of the deployment (it defaults to the class name), or the HTTP route prefix to expose the deployment at. See the Deployment API reference for more details. In order to deploy this, we simply need to call Counter.deploy() . Counter . deploy () Now that our deployment is up and running, let's test it out by making a query over HTTP. In your browser, simply visit http://127.0.0.1:8000/Counter , and you should see the output {\"count\": 1\"} . If you keep refreshing the page, the count should increase, as expected. Now let's say we want to update this deployment to add another method to decrement the counter. Here, because we want more flexible HTTP configuration we'll use Serve's FastAPI integration. For more information on this, please see serve-fastapi-http . from fastapi import FastAPI app = FastAPI () @serve . deployment @serve . ingress ( app ) class Counter : def __init__ ( self ): self . count = 0 @app . get ( \"/\" ) def get ( self ): return { \"count\" : self . count } @app . get ( \"/incr\" ) def incr ( self ): self . count += 1 return { \"count\" : self . count } @app . get ( \"/decr\" ) def decr ( self ): self . count -= 1 return { \"count\" : self . count } We've now redefined the Counter class to wrap a FastAPI application. This class is exposing three HTTP routes: /Counter will get the current count, /Counter/incr will increment the count, and /Counter/decr will decrement the count. To redeploy this updated version of the Counter , all we need to do is run Counter.deploy() again. Serve will perform a rolling update here to replace the existing replicas with the new version we defined. Counter . deploy () If we test out the HTTP endpoint again, we can see this in action. Note that the count has been reset to zero because the new version of Counter was deployed. > curl -X GET localhost:8000/Counter/ { \"count\" : 0 } > curl -X GET localhost:8000/Counter/incr { \"count\" : 1 } > curl -X GET localhost:8000/Counter/decr { \"count\" : 0 } Congratulations, you just built and ran your first Ray Serve application!","title":"Quickstart"},{"location":"ray-ml/ray-serve/tutorials/model-composition/","text":"Model Composition Ray Serve supports composing individually scalable models into a single model out of the box. For instance, you can combine multiple models to perform stacking or ensembles. To define a higher-level composed model you need to do three things: Define your underlying models (the ones that you will compose together) as Ray Serve deployments. Define your composed model, using the handles of the underlying models (see the example below). Define a deployment representing this composed model and query it! In order to avoid synchronous execution in the composed model (e.g., it's very slow to make calls to the composed model), you'll need to make the function asynchronous by using an async def . You'll see this in the example below. That's it. Let's take a look at an example: from random import random import requests import ray from ray import serve ray . init ( num_cpus = 8 ) serve . start () # Our pipeline will be structured as follows: # - Input comes in, the composed model sends it to model_one # - model_one outputs a random number between 0 and 1, if the value is # greater than 0.5, then the data is sent to model_two # - otherwise, the data is returned to the user. # Let's define two models that just print out the data they received. @serve . deployment def model_one ( data ): print ( \"Model 1 called with data \" , data ) return random () model_one . deploy () @serve . deployment def model_two ( data ): print ( \"Model 2 called with data \" , data ) return data model_two . deploy () # max_concurrent_queries is optional. By default, if you pass in an async # function, Ray Serve sets the limit to a high number. @serve . deployment ( max_concurrent_queries = 10 , route_prefix = \"/composed\" ) class ComposedModel : def __init__ ( self ): self . model_one = model_one . get_handle () self . model_two = model_two . get_handle () # This method can be called concurrently! async def __call__ ( self , starlette_request ): data = await starlette_request . body () score = await self . model_one . remote ( data = data ) if score > 0.5 : result = await self . model_two . remote ( data = data ) result = { \"model_used\" : 2 , \"score\" : score } else : result = { \"model_used\" : 1 , \"score\" : score } return result ComposedModel . deploy () for _ in range ( 5 ): resp = requests . get ( \"http://127.0.0.1:8000/composed\" , data = \"hey!\" ) print ( resp . json ()) # Output # {'model_used': 2, 'score': 0.6250189863595503} # {'model_used': 1, 'score': 0.03146855349621436} # {'model_used': 2, 'score': 0.6916977560006987} # {'model_used': 2, 'score': 0.8169693450866928} # {'model_used': 2, 'score': 0.9540681979573862}","title":"Model Composition"},{"location":"ray-ml/ray-serve/tutorials/model-composition/#model-composition","text":"Ray Serve supports composing individually scalable models into a single model out of the box. For instance, you can combine multiple models to perform stacking or ensembles. To define a higher-level composed model you need to do three things: Define your underlying models (the ones that you will compose together) as Ray Serve deployments. Define your composed model, using the handles of the underlying models (see the example below). Define a deployment representing this composed model and query it! In order to avoid synchronous execution in the composed model (e.g., it's very slow to make calls to the composed model), you'll need to make the function asynchronous by using an async def . You'll see this in the example below. That's it. Let's take a look at an example: from random import random import requests import ray from ray import serve ray . init ( num_cpus = 8 ) serve . start () # Our pipeline will be structured as follows: # - Input comes in, the composed model sends it to model_one # - model_one outputs a random number between 0 and 1, if the value is # greater than 0.5, then the data is sent to model_two # - otherwise, the data is returned to the user. # Let's define two models that just print out the data they received. @serve . deployment def model_one ( data ): print ( \"Model 1 called with data \" , data ) return random () model_one . deploy () @serve . deployment def model_two ( data ): print ( \"Model 2 called with data \" , data ) return data model_two . deploy () # max_concurrent_queries is optional. By default, if you pass in an async # function, Ray Serve sets the limit to a high number. @serve . deployment ( max_concurrent_queries = 10 , route_prefix = \"/composed\" ) class ComposedModel : def __init__ ( self ): self . model_one = model_one . get_handle () self . model_two = model_two . get_handle () # This method can be called concurrently! async def __call__ ( self , starlette_request ): data = await starlette_request . body () score = await self . model_one . remote ( data = data ) if score > 0.5 : result = await self . model_two . remote ( data = data ) result = { \"model_used\" : 2 , \"score\" : score } else : result = { \"model_used\" : 1 , \"score\" : score } return result ComposedModel . deploy () for _ in range ( 5 ): resp = requests . get ( \"http://127.0.0.1:8000/composed\" , data = \"hey!\" ) print ( resp . json ()) # Output # {'model_used': 2, 'score': 0.6250189863595503} # {'model_used': 1, 'score': 0.03146855349621436} # {'model_used': 2, 'score': 0.6916977560006987} # {'model_used': 2, 'score': 0.8169693450866928} # {'model_used': 2, 'score': 0.9540681979573862}","title":"Model Composition"},{"location":"ray-ml/ray-train/getting-started/","text":"getting started","title":"Getting started"},{"location":"ray-ml/ray-train/key-concepts/","text":"key_concepts","title":"Key concepts"},{"location":"ray-ml/ray-train/tutorials/","text":"tutorials","title":"Tutorials"},{"location":"ray-ml/ray-tune/getting-started/","text":"getting started","title":"Getting started"},{"location":"ray-ml/ray-tune/key-concepts/","text":"key_concepts","title":"Key concepts"},{"location":"ray-ml/ray-tune/tutorials/","text":"tutorials","title":"Tutorials"},{"location":"ray-ml/ray-workflows/getting-started/","text":"getting started","title":"Getting started"},{"location":"ray-ml/ray-workflows/key-concepts/","text":"key_concepts","title":"Key concepts"},{"location":"ray-ml/ray-workflows/tutorials/","text":"tutorials","title":"Tutorials"}]}